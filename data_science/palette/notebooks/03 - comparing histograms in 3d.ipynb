{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3D colour space \n",
    "As we've seen in previous notebooks, we can explore an image by the position of its pixels in an n-dimensional colour space. The 3D RGB space can be chopped up into sections (or bins) as follows:\n",
    "\n",
    "![RGB space](https://upload.wikimedia.org/wikipedia/commons/thumb/0/05/RGB_Cube_Show_lowgamma_cutout_a.png/1280px-RGB_Cube_Show_lowgamma_cutout_a.png)\n",
    "\n",
    "By counting number of pixels appearing in each bin, the underlying pixel positions remain continuous, but the view we obtain from the reduced space is more computationally manageable (a $(16 \\times 16 \\times 16)$ binning of the space produces 4096 degrees of freedom, while the original $(256 \\times 256 \\times 256)$ gives us 16777216 to deal with). It also gives a more intuitive, blurred view of the similarity of neighbouring colours to one another. If we can find a way of computing the similarity of the binned spaces for two images, we should be well on our way.\n",
    "\n",
    "Let's start by loading in some images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "sns.set_style('white')\n",
    "plt.rcParams['figure.figsize'] = (20, 20)\n",
    "\n",
    "import os\n",
    "import itertools\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "from skimage.color import rgb2lab\n",
    "from sklearn.metrics.pairwise import pairwise_distances\n",
    "from umap import UMAP\n",
    "\n",
    "from tqdm import tqdm_notebook as tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_images = 5000\n",
    "path_to_images = '../data/small_images/'\n",
    "\n",
    "random_ids = np.random.choice(os.listdir(path_to_images), \n",
    "                              n_images, \n",
    "                              replace=False)\n",
    "\n",
    "image_dict = {}\n",
    "for image_id in tqdm(random_ids):\n",
    "    try: \n",
    "        image = Image.open(path_to_images + image_id)\n",
    "        if len(np.array(image).shape) != 3:\n",
    "            image = Image.fromarray(np.stack((image,)*3, -1))\n",
    "        image_dict[image_id] = image\n",
    "    except: \n",
    "        pass\n",
    "\n",
    "image_ids = list(image_dict.keys())\n",
    "images = list(image_dict.values())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll now resize and reshape the data for each image. First we shrink the image to a small size (just $75 \\times 75$ pixels!), before stacking them all into a long, flat array of pixel 3-vectors. For simplicity later on, we'll also join up each pixel-array with its corresponding `image_id` in a dictionary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "small_size = 75\n",
    "\n",
    "pixel_lists = [(np.array(image.resize((small_size, small_size),\n",
    "                                      resample=Image.BILINEAR))\n",
    "                .reshape(-1, 3)) for image in images]\n",
    "\n",
    "pixel_dict = dict(zip(image_ids, pixel_lists))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's worth emphasizing how important the resizing is at this stage. The original images (already shrunk down to a max width/height of 500px) can contain hundreds of thousands of pixels, and processing that much data for each of our thousands of images makes the next stage of the process _painfully_ slow. $75 \\times 75$ might seem small, but it seems to provide enough detail to get an impression of an image's dominant colours while keeping the subsequent processing speedy.\n",
    "\n",
    "### Binning\n",
    "In the next step, we split our colour-space into an even grid of bins and count the number of pixels appearing in each. Note that the value of `n_bins` seems to have a large effect on the 'goodness' of the results, and a higher granularity does not necessarily lead to better results. I've currently settled on 10 as a reasonable number, but I'm sure this could be more intelligently optimised.   \n",
    "Binning an image's pixels provides us with a rough, grainy distribution of area that it occupies in colour-space. The counts are then flattened into an 1D array which we'll use to compare the image to its counterparts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_bins = 16\n",
    "bin_counts = pd.DataFrame()\n",
    "\n",
    "for image_id, image in tqdm(pixel_dict.items()):\n",
    "    binned_pixels = (image / n_bins).astype(np.uint8).tolist()\n",
    "    bin_strings = list(map(str, binned_pixels))\n",
    "    unique, counts = np.unique(bin_strings, return_counts=True)\n",
    "    bin_counts[image_id] = pd.Series(dict(zip(unique, counts)))\n",
    "\n",
    "bin_counts = bin_counts.fillna(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dimensionality reduction\n",
    "It's always nice to visualise the separation of vectors within a newly defined feature space - let's do that here with UMAP."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding = UMAP().fit_transform(bin_counts.T.values)\n",
    "\n",
    "plt.scatter(x=embedding[:, 0], \n",
    "            y=embedding[:, 1]);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Similarity\n",
    "Now that we have a colour vector for each image, we'll compare them all to one another and store the numeric similarities in a great big dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "similarity = pd.DataFrame(data=pairwise_distances(bin_counts.T, \n",
    "                                                  metric='cosine'),\n",
    "                          index=bin_counts.columns,\n",
    "                          columns=bin_counts.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.heatmap(similarity);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Search \n",
    "We'll only really know the goodness of the results by running a search with a few randomly chosen query images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_id = np.random.choice(bin_counts.columns)\n",
    "image_dict[query_id]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "resolution = 200\n",
    "n_similar = 25\n",
    "size = int(n_similar ** 0.5)\n",
    "height = int(resolution * size)\n",
    "width = int(resolution * size)\n",
    "\n",
    "big_image = np.empty((height, width, 3)).astype(np.uint8)\n",
    "grid = np.array(list(itertools.product(range(size), range(size))))\n",
    "\n",
    "most_similar_ids = similarity[query_id].sort_values().index.values[1 : n_similar + 1]\n",
    "similar_images = [image_dict[id].resize((resolution, resolution),\n",
    "                                        resample=Image.BILINEAR) \n",
    "                  for id in most_similar_ids]\n",
    "\n",
    "for pos, image in zip(grid, similar_images):\n",
    "    block_t, block_l = pos * resolution\n",
    "    block_b, block_r = (pos + 1) * resolution\n",
    "    \n",
    "    big_image[block_t : block_b, block_l : block_r] = np.array(image)\n",
    "\n",
    "Image.fromarray(big_image)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The results here are definitely better than those produced by previous approaches, but the hard boundary between bins is a bit ugly and frustrating so I'd like to carry on in search of the perfect technique."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
