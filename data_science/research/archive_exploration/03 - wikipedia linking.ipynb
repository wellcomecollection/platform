{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A couple of brief notes on how archives store information\n",
    "Archives are arranged in a really interesting way - they're roughly hierarchical trees (with some more graphy elements when you get close to the leaf nodes). Each node contains multiple free-text notes fields which provide descriptions of the common features of their children (this applies even at leaf node level - an _item_ in the hierarchy is usually a box containing multiple _pieces_ of physical material, ie multiple letters or multiple notebook pages). The tree doesn't _necessarily_ describe the arrangememt of nested items in physical space; The tree is more like an _informational_ hierarchy where sections, series and items are gathered conceptually.\n",
    "\n",
    "This sounds great in theory but, as we saw in the last notebook, the tree-based structure makes it difficult to jump quickly from a leaf node in one branch of the hierarchy to another (conceptually related) leaf existing in another branch. Making the jump between the nodes would involve traversing up the tree to (at least) the point where the branches split, and then travelling all the way back down to the related leaf. That journey also requires perfect knowledge of the tree's structure and an understanding of the complete context. While archivists do a good job arranging items into conceptual hierarchies, information is incredibly messy medium to work with and its full complexity often can't be ecapsulated by the tree structure. Archivists therefore often default to the items' _provenance_ as a way of overcoming the problem while conforming to the established guidelines. _Provenance_ refers to the way that the archive material was arranged by its original owner. In this way, archives preserve the owner's usage (a good and valuable thing), but that benefit comes at a cost to users trying to discover information or make connections within/across archives\n",
    "\n",
    "# Wikipedia linking\n",
    "Arriving at a leaf node, it can be hard to find its context within the collection without full tree traversal. \n",
    "\n",
    "Fun idea - wouldn't it be nice to take any arbitrary record from the archive and automatically annotate it with links to relevant wikipedia articles? Can we make our archive records look and feel more like wikipedia pages?  \n",
    "Wikipedia's strength is in its internal links - a typical page contains dozens of links to other contextually related pages, allowing users to traverse an endless warren of information. It's a fantastic embodiment of the idealistic early internet - utopian visions of hypertext, linked open data, and . It's a great established model and it _works_, so why not emulate it?   \n",
    "Wellcome has great links with wikipedia/wikimedia/wikidata, and a lot of our digitised material has already ended up on their platform(s). Why not use the incredible graph that is wikidata to enhance our archives, and in turn enhance wikidata when the archive data eventually makes its way there?  \n",
    "Ideally we'll be able to intelligently link all of our own archive data to itself eventually, but we can make a quick, cheap start by tying our records to wikipedia first.\n",
    "\n",
    "### Loading data\n",
    "As usual, we'll start by importing a few useful packages for manipulating and displaying the data, and load up the data itself."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "from string import punctuation\n",
    "import spacy\n",
    "import re\n",
    "from IPython.core.display import display, HTML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_json('data/calm_records.json')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Abortion Laws Reform Act\n",
    "An interesting example to start with"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "record = df.loc[269057]['AdminHistory'][0]\n",
    "record[:1000]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "this record contains a load of ugly hard-coded HTML - let's parse that and turn it into more readable plaintext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "soup = BeautifulSoup(record, 'html.parser')\n",
    "plain_text = soup.get_text()\n",
    "\n",
    "print(plain_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`Spacy` is a nice natural language processing (NLP) library which rapidly adds tonnes of metadata to a document. Each word is automatically tagged with a part-of-speech tag, a word vector etc. Without the user having to do anything at all, spacy will do 95% of the usual pre-processing required for typical NLP tasks. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load('en')\n",
    "doc = nlp(plain_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That's all we need to do...\n",
    "\n",
    "We can use now use spacy's (POS) tags to identify _named entities_, like people, places, or organisations. To a decent approximation, these named entities are usually the words which wikipedia chooses to provide more context to with a link. By identifying the relevant named entities (excluding certain types, see more documentation of entity types [here](https://spacy.io/usage/linguistic-features#entity-types)), we can do a tiny amount of string manipulation and return a neat wikipedia search string."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ent_types = ['PERSON', \n",
    "             'NORP', \n",
    "             'FACILITY', \n",
    "             'ORG', \n",
    "             'GPE', \n",
    "             'LOC', \n",
    "             'PRODUCT', \n",
    "             'EVENT', \n",
    "             'WORK_OF_ART', \n",
    "             'LAW', \n",
    "             'LANGUAGE']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for ent in doc.ents:\n",
    "    if ent.label_ in ent_types and len(ent.text.split()) > 1:\n",
    "        words = ent.text.lower().split()\n",
    "        words = [word.replace(\"'s\", '') for word in words]\n",
    "        words = [word.translate(str.maketrans('', '', punctuation)) \n",
    "                 for word in words]\n",
    "\n",
    "        print('https://en.wikipedia.org/w/index.php?search=' + \n",
    "              '+'.join(words))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wikipedia's search URLs are great. If wikipedia spots that the search returns a unique result, the user is seamlessly redirected to that result's page. If multiple results are close to the search string, a disambiguation page is returned. If the search is rubbish, the raw search page is returned with a typical list of search results. Try a few of the links above and see which kinds of search work better than others.  \n",
    "We'll now follow the same process as above and dump each link (wrapped with a little HTML) into a dictionary, keyed by their original plaintext strings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "links = {}\n",
    "\n",
    "for ent in doc.ents:\n",
    "    if ent.label_ in ent_types and len(ent.text.split()) > 1:\n",
    "        words = ent.text.lower().split()\n",
    "        words = [word.translate(str.maketrans('', '', punctuation)) \n",
    "                 for word in words]\n",
    "        url = ('https://en.wikipedia.org/w/index.php?search=' + \n",
    "                '+'.join(words))\n",
    "        link = '<a href=\"{}\">{}</a>'.format(url, ent.text.strip())\n",
    "        links[ent.text.strip()] = link"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now perform a super basic regex replacement. We're looking for the original plaintext strings which were recognised as relevant named entities, and replacing them with HTML links to the wikipedia searches. If the search is decent, the reader will be pointed straight to a page of additional contextual information, deepening their understanding of the subject matter much faster than the arduous archive traversal process described above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pattern = re.compile(r'\\b(' + '|'.join(links.keys()) + r')\\b')\n",
    "result = pattern.sub(lambda x: links[x.group()], str(soup))\n",
    "\n",
    "display(HTML(result))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Admittedly this process is still far from perfect, and the flaws start to appear as soon as you click more than a few basic links... Spacy's NER algorithm is okay at best, and the wikipedia search only works well when it's provided with a clean, unambiguous string. There's a lot to improve here, and we can be much smarter about the way we make use of wikidata (see notebooks to follow), but for the work of an hour on a Friday afternoon, this isn't bad..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
