{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Archive data\n",
    "The Wellcome archive sits in a collections management system called CALM, which follows a rough set of standards and guidelines for storing archival records called [ISAD(G)](https://en.wikipedia.org/wiki/ISAD(G). The archive is comprised of _collections_, each of which has a hierarchical set of series, sections, subjects, items and pieces sitting underneath it.  \n",
    "In the following notebooks I'm going to explore it and try to make as much sense of it as I can programatically.\n",
    "\n",
    "Let's start by loading in a few useful packages and defining some nice utils."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "sns.set_style('white')\n",
    "plt.rcParams['figure.figsize'] = (20, 20)\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import networkx as nx\n",
    "\n",
    "from sklearn.cluster import AgglomerativeClustering\n",
    "\n",
    "from umap import UMAP\n",
    "from tqdm import tqdm_notebook as tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def flatten(input_list):\n",
    "    return [item \n",
    "            for sublist in input_list \n",
    "            for item in sublist]\n",
    "\n",
    "def cartesian(*arrays):\n",
    "    return np.array([x.reshape(-1) for x in np.meshgrid(*arrays)]).T\n",
    "\n",
    "def clean(subject):\n",
    "    return subject.strip().lower().replace('<p>', '')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "let's load up our CALM data. The data has been exported in its entirety as a single `.json`  where each line is a record.  \n",
    "You can download the data yourself using [this script](https://github.com/wellcometrust/platform/blob/master/misc/download_oai_harvest.py). Stick the `.json` in the neighbouring `/data` directory to run the rest of the notebook seamlessly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_json('data/calm_records.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.astype(str).describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exploring individual columns\n",
    "At the moment I have no idea what kind of information CALM contains - lets look at the list of column names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here I'm looking through a sample of values in each column, choosing the columns to explore based on the their headings, a bit of contextual info from colleagues and the `df.describe()` above. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Subject']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### After much trial and error...\n",
    "Subjects look like an interesting avenue to explore further. Where subjects have _actually_ been filled in and the entry is not `None`, a list of subjects is returned.  \n",
    "We can explore some of these subjects' subtleties by creating an adjacency matrix. We'll count the number of times each subject appears alongside every other subject and return a big $n \\times n$ matrix, where $n$ is the total number of unique subjects.  \n",
    "We can use this adjacency matrix for all sorts of stuff, but we have to build it first. To start, lets get a uniqur list of all subjects. This involves unpacking each sub-list and flattening them out into one long list, before finding the unique elements. We'll also use the `clean` function defined above to get rid of any irregularities which might become annoying later on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subjects = flatten(df['Subject'].dropna().tolist())\n",
    "print(len(subjects))\n",
    "subjects = list(set(map(clean, subjects)))\n",
    "print(len(subjects))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At this point it's often helpful to index our data, ie transform words into numbers. We'll create two dictionaries which map back and forth between the subjects and their corresponding indicies:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "index_to_subject = {index: subject for index, subject in enumerate(subjects)}\n",
    "subject_to_index = {subject: index for index, subject in enumerate(subjects)}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets instantiate an empty numpy array which we'll then fill with our coocurrence data. Each column and each row will represent a subject - each cell (the intersection of a column and row) will therefore represent the 'strength' of the interaction between those subjects. As we haven't seen any interactions yet, we'll set every array element to 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "adjacency = np.empty((len(subjects), len(subjects)), \n",
    "                     dtype=np.uint16)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To populate the matrix, we want to find every possible combination of subject in each sub-list from our original column, ie if we had the subjects\n",
    "\n",
    "`[Disease, Heart, Heart Diseases, Cardiology]`\n",
    "\n",
    "we would want to return \n",
    "\n",
    "`\n",
    "[['Disease', 'Disease'],\n",
    " ['Heart', 'Disease'],\n",
    " ['Heart Diseases', 'Disease'],\n",
    " ['Cardiology', 'Disease'],\n",
    " ['Disease', 'Heart'],\n",
    " ['Heart', 'Heart'],\n",
    " ['Heart Diseases', 'Heart'],\n",
    " ['Cardiology', 'Heart'],\n",
    " ['Disease', 'Heart Diseases'],\n",
    " ['Heart', 'Heart Diseases'],\n",
    " ['Heart Diseases', 'Heart Diseases'],\n",
    " ['Cardiology', 'Heart Diseases'],\n",
    " ['Disease', 'Cardiology'],\n",
    " ['Heart', 'Cardiology'],\n",
    " ['Heart Diseases', 'Cardiology'],\n",
    " ['Cardiology', 'Cardiology']]\n",
    "`\n",
    "\n",
    "The `cartesian()` function which I've defined above will do that for us. We then find the appropriate intersection in the matrix and add another unit of 'strength' to it.  \n",
    "We'll do this for every row of subjects in the `['Subjects']` column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for row_of_subjects in tqdm(df['Subject'].dropna()):\n",
    "    for subject_pair in cartesian(row_of_subjects, row_of_subjects):\n",
    "        subject_index_1 = subject_to_index[clean(subject_pair[0])]\n",
    "        subject_index_2 = subject_to_index[clean(subject_pair[1])]\n",
    "\n",
    "        adjacency[subject_index_1, subject_index_2] += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can do all sorts of fun stuff now - adjacency matrices are the foundation on which all of graph theory is built. However, because it's a bit more interesting, I'm going to start with some dimensionality reduction. We'll get to the graphy stuff later.  \n",
    "Using [UMAP](https://github.com/lmcinnes/umap), we can squash the $n \\times n$ dimensional matrix down into a $n \\times m$ dimensional one, where $m$ is some arbitrary integer. Setting $m$ to 2 will then allow us to plot each subject as a point on a two dimensional plane. UMAP will try to preserve the 'distances' between subjects - in this case, that means that related or topically similar subjects will end up clustered together, and different subjects will move apart."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_2d = pd.DataFrame(UMAP(n_components=2)\n",
    "                            .fit_transform(adjacency))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_2d.plot.scatter(x=0, y=1);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can isolate the clusters we've found above using a number of different methods - `scikit-learn` provides easy access to some very powerful algorithms. Here I'll use a technique called _agglomerative clustering_, and make a guess that 15 is an appropriate number of clusters to look for."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_clusters = 15\n",
    "\n",
    "embedding_2d['labels'] = (AgglomerativeClustering(n_clusters)\n",
    "                          .fit_predict(embedding_2d.values))\n",
    "                          \n",
    "embedding_2d.plot.scatter(x=0, y=1, \n",
    "                          c='labels', \n",
    "                          cmap='Paired');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now use the `index_to_subject` mapping that we created earlier to examine which subjects have been grouped together into clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(n_clusters):\n",
    "    print(str(i) + ' ' + '-'*80 + '\\n')\n",
    "    print(np.sort([index_to_subject[index]\n",
    "                   for index in embedding_2d[embedding_2d['labels'] == i].index.values]))\n",
    "    print('\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Interesting! Taking a look at some of the smaller clusters of subjects (for the sake of space and your willingness to read lists of 100s of subjects):\n",
    "\n",
    "One seems to be quite distinctly involved with drugs and associated topics/treatments:\n",
    "```\n",
    "13 --------------------------------------------------------------------------------\n",
    "\n",
    "['acquired immunodeficiency syndrome' 'alcohol' 'amphetamines'\n",
    " 'analgesics, opioid' 'campaign' 'cannabis' 'cocaine' 'counseling'\n",
    " 'counterculture' 'crime' 'drugs' 'education' 'hallucinogens' 'heroin'\n",
    " 'hypnotics and sedatives' 'information services' 'inhalant abuse'\n",
    " 'lysergic acid diethylamide' 'n-methyl-3,4-methylenedioxyamphetamine'\n",
    " 'opioid' 'policy' 'prescription drugs' 'rehabilitation' 'renabilitation'\n",
    " 'self-help']\n",
    "```\n",
    "\n",
    "others are linked to early/fundamental research on DNA and genetics:\n",
    "\n",
    "```\n",
    "9 --------------------------------------------------------------------------------\n",
    "\n",
    "['bacteriophages' 'biotechnology' 'caenorhabditis elegans'\n",
    " 'chromosome mapping' 'cloning, organism' 'discoveries in science' 'dna'\n",
    " 'dna, recombinant' 'genetic code' 'genetic engineering'\n",
    " 'genetic research' 'genetic therapy' 'genome, human' 'genomics'\n",
    " 'magnetic resonance spectroscopy' 'meiosis' 'models, molecular'\n",
    " 'molecular biology' 'nobel prize' 'retroviridae' 'rna'\n",
    " 'sequence analysis' 'viruses']\n",
    "```\n",
    "\n",
    "and others about food\n",
    "```\n",
    "14 --------------------------------------------------------------------------------\n",
    "\n",
    "['acids' 'advertising' 'ambergris' 'animals' 'beer' 'biscuits' 'brassica'\n",
    " 'bread' 'butter' 'cacao' 'cake' 'candy' 'carbohydrates' 'cattle'\n",
    " 'cereals' 'cheese' 'chemistry, agricultural' 'cider' 'colouring agents'\n",
    " 'condiments' 'cooking (deer)' 'cooking (poultry)' 'cooking (venison)'\n",
    " 'cucumis sativus' 'dairy products' 'daucus carota' 'desserts'\n",
    " 'dried fruit' 'ecology' 'economics' 'eggs' 'environmental health'\n",
    " 'european rabbit' 'fermentation' 'food additives' 'food and beverages'\n",
    " 'food preservation' 'food, genetically modified' 'fruit' 'fruit drinks'\n",
    " 'fungi' 'game and game-birds' 'grapes' 'hands' 'health attitudes'\n",
    " 'herbaria' 'honey' 'jam' 'legislation' 'lettuce' 'meat' 'meat products'\n",
    " 'nuts' 'oatmeal' 'olive' 'onions' 'peas' 'pickles' 'pies' 'poultry'\n",
    " 'preserves (jams)' 'puddings' 'rice' 'seafood' 'seeds' 'sheep'\n",
    " 'sociology' 'solanum tuberosum' 'spinacia oleracea' 'sweetening agents'\n",
    " 'swine' 'syrups' 'vegetables' 'vitis' 'whiskey' 'wild flowers' 'wine']\n",
    "```\n",
    "\n",
    "These are all noticeably different themes, and they appear to be nicely separated in the topic-space we've built."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
