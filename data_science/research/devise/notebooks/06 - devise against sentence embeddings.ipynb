{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "sns.set_style('whitegrid')\n",
    "plt.rcParams['figure.figsize'] = (20, 20)\n",
    "\n",
    "import os\n",
    "import json\n",
    "import nltk\n",
    "import numpy as np \n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "from scipy.spatial.distance import cdist\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "\n",
    "import torch\n",
    "from torch import nn, optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import models, transforms\n",
    "\n",
    "nltk.download('punkt')\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# load coco images and captions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('/mnt/efs/images/coco/annotations/captions_val2014.json') as f:\n",
    "    meta = json.load(f)\n",
    "\n",
    "df = (pd.merge(pd.DataFrame(meta['images']).set_index('id'),\n",
    "               pd.DataFrame(meta['annotations']).set_index('image_id'), \n",
    "               left_index=True, right_index=True)\n",
    "      .reset_index()\n",
    "      [['caption', 'file_name']]\n",
    "     )\n",
    "\n",
    "df['file_name'] = '/mnt/efs/images/coco/val2014/' + df['file_name']\n",
    "\n",
    "df['caption'] = (df['caption']\n",
    "                 .apply(lambda x: ''.join([c for c in x if c.isalpha() or c.isspace()]))\n",
    "                 .apply(str.lower)\n",
    "                 .apply(lambda x: ' '.join(x.split()))\n",
    "                )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# train test splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "split_ratio = 0.8\n",
    "train_size = int(split_ratio * len(df))\n",
    "\n",
    "train_df = df.loc[:train_size]\n",
    "test_df  = df.loc[train_size:]\n",
    "len(train_df), len(test_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# load InferSent model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from InferSent import InferSent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_PATH =  '/mnt/efs/models/infersent2.pkl'\n",
    "\n",
    "params_model = {'bsize': 1024, \n",
    "                'word_emb_dim': 300, \n",
    "                'enc_lstm_dim': 2048,\n",
    "                'pool_type': 'max', \n",
    "                'dpout_model': 0.0, \n",
    "                'version': 2}\n",
    "\n",
    "infersent_model = InferSent(params_model)\n",
    "infersent_model.load_state_dict(torch.load(MODEL_PATH))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "W2V_PATH = '/mnt/efs/nlp/word_vectors/fasttext/crawl-300d-2M.vec'\n",
    "infersent_model.set_w2v_path(W2V_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "infersent_model.build_vocab_k_words(K=100000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "infersent_model = infersent_model.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# embed captions with infersent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_embeddings = infersent_model.encode(train_df['caption'].values, tokenize=True)\n",
    "test_embeddings = infersent_model.encode(test_df['caption'].values, tokenize=True)\n",
    "\n",
    "len(train_embeddings), len(test_embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# pytorch datasets and dataloaders\n",
    "\n",
    "### dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CaptionsDataset(Dataset):\n",
    "    def __init__(self, path_df, caption_embeddings, \n",
    "                 transform=transforms.ToTensor()):\n",
    "        self.ids = path_df.index.values\n",
    "        self.image_paths = path_df['file_name'].values\n",
    "        self.titles = path_df['caption'].values\n",
    "        self.caption_embeddings = caption_embeddings\n",
    "        self.transform = transform\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        image = Image.open(self.image_paths[index]).convert('RGB')\n",
    "        if self.transform is not None:\n",
    "            image = self.transform(image)\n",
    "\n",
    "        target = self.caption_embeddings[index]\n",
    "        return image, target\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose([transforms.RandomResizedCrop(224, scale=[0.5, 0.9]),\n",
    "                                transforms.RandomHorizontalFlip(),\n",
    "                                transforms.RandomGrayscale(0.25),\n",
    "                                transforms.ToTensor()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = CaptionsDataset(train_df, train_embeddings, transform=transform)\n",
    "test_dataset = CaptionsDataset(test_df, test_embeddings, transform=transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset.__getitem__(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 128\n",
    "\n",
    "train_loader = DataLoader(dataset=train_dataset, \n",
    "                          batch_size=batch_size,\n",
    "                          shuffle=True,\n",
    "                          num_workers=5)\n",
    "\n",
    "test_loader = DataLoader(dataset=test_dataset, \n",
    "                         batch_size=batch_size,\n",
    "                         num_workers=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# create DeViSE model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "backbone = models.vgg16_bn(pretrained=True).features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for param in backbone[:34].parameters():\n",
    "    param.requires_grad = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DeViSE(nn.Module):\n",
    "    def __init__(self, backbone, target_size=300):\n",
    "        super(DeViSE, self).__init__()\n",
    "        self.backbone = backbone\n",
    "        self.head = nn.Sequential(\n",
    "            nn.Linear(in_features=(25088), out_features=target_size*2),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Dropout(),\n",
    "            nn.Linear(in_features=target_size*2, out_features=target_size),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Dropout(),\n",
    "            nn.Linear(in_features=target_size, out_features=target_size),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.backbone(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.head(x)\n",
    "        x = x / x.max()\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "devise_model = DeViSE(backbone, target_size=4096).to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "losses = []\n",
    "\n",
    "def train(model, train_loader, n_epochs, loss_function, \n",
    "          additional_metric, optimiser, device=device):\n",
    "    '''\n",
    "    do some training\n",
    "    '''\n",
    "    model.train()\n",
    "    for epoch in range(n_epochs):\n",
    "        loop = tqdm(train_loader)\n",
    "        for data, target in loop:\n",
    "            data, target, flags = (data.cuda(non_blocking=True), \n",
    "                                   target.cuda(non_blocking=True), \n",
    "                                   torch.ones(len(target)).cuda(non_blocking=True))\n",
    "\n",
    "            optimiser.zero_grad()\n",
    "            prediction = model(data)\n",
    "\n",
    "            loss = loss_function(prediction, target, flags)\n",
    "            mean_sq_error = additional_metric(prediction, target)\n",
    "            losses.append([loss.item(), mean_sq_error.item()])\n",
    "\n",
    "            loss.backward()\n",
    "            optimiser.step()\n",
    "\n",
    "            loop.set_description('Epoch {}/{}'.format(epoch + 1, n_epochs))\n",
    "            loop.set_postfix(loss=loss.item(), mse=mean_sq_error.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.backends.cudnn.benchmark = True\n",
    "\n",
    "trainable_parameters = filter(lambda p: p.requires_grad, devise_model.parameters())\n",
    "\n",
    "loss_function, mse = nn.CosineEmbeddingLoss(), nn.MSELoss()\n",
    "optimiser = optim.Adam(trainable_parameters, lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train(model=devise_model,\n",
    "      train_loader=train_loader,\n",
    "      loss_function=loss_function,\n",
    "      additional_metric=mse, \n",
    "      optimiser=optimiser,\n",
    "      n_epochs=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_data = pd.DataFrame(losses).rolling(window=15).mean()\n",
    "loss_data.columns = ['cosine loss', 'mse']\n",
    "ax = loss_data.plot(subplots=True);\n",
    "\n",
    "ax[0].set_xlim(0,);\n",
    "ax[0].set_ylim(0.3, 0.6);\n",
    "ax[1].set_ylim(0,);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# evaluate on test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = []\n",
    "test_loss = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    test_loop = tqdm(test_loader)\n",
    "    for data, target in test_loop:\n",
    "        data, target, flags = (data.cuda(non_blocking=True),\n",
    "                               target.cuda(non_blocking=True),\n",
    "                               torch.ones(len(target)).cuda(non_blocking=True))\n",
    "\n",
    "        prediction = devise_model.eval()(data)\n",
    "        loss = loss_function(prediction, target, flags)\n",
    "\n",
    "        preds.append(prediction.cpu().data.numpy())\n",
    "        test_loss.append(loss.item())\n",
    "\n",
    "        test_loop.set_description('Test set')\n",
    "        test_loop.set_postfix(loss=loss.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = np.concatenate(preds).reshape(-1, 4096)\n",
    "np.mean(test_loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# run a test search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def search(query):\n",
    "    query_embedding = infersent_model.encode([query], tokenize=True)\n",
    "\n",
    "    distances = cdist(query_embedding, preds, 'cosine').squeeze()\n",
    "    nearby_image_paths = test_df['file_name'].values[np.argsort(distances)][:20]\n",
    "    nearby_images = [np.array((Image.open(path)\n",
    "                               .convert('RGB')\n",
    "                               .resize((224, 224), Image.BILINEAR)))\n",
    "                     for path in nearby_image_paths]\n",
    "\n",
    "    return Image.fromarray(np.concatenate([np.concatenate(nearby_images[:5], axis=1),\n",
    "                                           np.concatenate(nearby_images[5:10], axis=1),\n",
    "                                           np.concatenate(nearby_images[10:15], axis=1),\n",
    "                                           np.concatenate(nearby_images[15:20], axis=1)],\n",
    "                                          axis=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "search('a man playing tennis')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
