{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# sentence embeddings via NLI from scratch\n",
    "Rather than using multiple tasks to induce generality in its sentence embeddings, InferSent takes on one very complex task, popularised by the SNLI dataset. The algorithm learns to categorise a pair of sentences (one following the other) as contraditions, entailments or neutral statements of one another. It's an absurdly subtle task, but InferSent and many other surprisingly simple deep learning systems have managed to take it on and achieve good classification performance and consequently producing meaningful sentence embeddings.  \n",
    "However, the SNLI dataset uses sequences of quite predictable length, which is why it struggles to interpret single- or double-word queries effectively. We need to embed queries, not sentences, so some modifications to the training data are necessary. We can supplement the SNLI dataset with MultiNLI (providing a broader range of language and context), COCO (a natural choice when working with image captions/search, where all combinatorial pairs of captions for the same image are treated as entailments), and sequences paired with nouns and adjective-noun pairs extracted from those sequences (again, all pairs treated as entailments). This should increase the granularity and dexterity of the embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "sns.set_style('whitegrid')\n",
    "plt.rcParams['figure.figsize'] = (20, 14)\n",
    "\n",
    "import os\n",
    "import json\n",
    "import nltk\n",
    "import spacy\n",
    "import itertools\n",
    "import numpy as np \n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "from scipy.spatial.distance import cdist\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "from tqdm import tqdm as tqdm_\n",
    "tqdm_.pandas()\n",
    "\n",
    "import io\n",
    "from nltk.tokenize import word_tokenize\n",
    "from sklearn.preprocessing import LabelEncoder, OneHotEncoder\n",
    "\n",
    "import torch\n",
    "from torch import nn, optim\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "from torchvision import models, transforms\n",
    "\n",
    "nlp = spacy.load('en')\n",
    "nltk.download('punkt')\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# assemble SNLI, MultiNLI and COCO dataframes\n",
    "loading in these datasets is fairly simple - the only complication is finding all of the combinatorial pairs of coco captions, but `itertools.combinations` makes this process much simpler."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "multinli = (pd.read_json('/mnt/efs/nlp/natural_language_inference/multinli_1.0/multinli_1.0_train.jsonl', \n",
    "                         lines=True)\n",
    "            [['gold_label', 'sentence1', 'sentence2']]\n",
    "           )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "snli = (pd.read_json('/mnt/efs/nlp/natural_language_inference/snli_1.0/snli_1.0_train.jsonl', \n",
    "                     lines=True)\n",
    "        [['gold_label', 'sentence1', 'sentence2']]\n",
    "       )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('/mnt/efs/nlp/natural_language_inference/coco2014/captions_train2014.json') as f:\n",
    "    df = pd.DataFrame(json.load(f)['annotations'])\n",
    "\n",
    "coco, i = {}, 0\n",
    "for image_id in tqdm(df['image_id'].unique()):\n",
    "    captions = df[df['image_id'] == image_id]['caption'].values\n",
    "    for s1, s2 in list(itertools.combinations(captions, 2)):\n",
    "        coco[i] = {'gold_label': 'entailment',\n",
    "                   'sentence1': s1, 'sentence2': s2}\n",
    "        i += 1\n",
    "\n",
    "coco = pd.DataFrame(coco).T\n",
    "del df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# find nouns and adjective-noun pairs in sentences\n",
    "The hope is that by conflating the individual subject nouns and adjective-noun pairs in our source data with their full sentence forms, the network will learn to represent them as essentially the same thing. We want our network to be as meaningful for single word queries as if we were to just use the simple 300d word-vector space, and this is the most straightforward way of doing that which I can imagine, without branching off again into multi-task learning.\n",
    "\n",
    "First we'll grab a few thousand random sequences from the original datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = (pd.concat([multinli, snli, coco])\n",
    "             .fillna('')\n",
    "             ['sentence1']\n",
    "             .sample(20000)\n",
    "             .values)\n",
    "\n",
    "subjects = {}\n",
    "i = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We extract the nouns from the sequence (using spacy's POS tagger) and add them to a dictionary, paired with their original sequence and an `'entailment'` label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for sentence in tqdm(sentences):\n",
    "    for word in nlp(sentence):\n",
    "        if word.pos_ == 'NOUN':\n",
    "            subjects[i] = {'sentence1': word.text,\n",
    "                           'sentence2': sentence,\n",
    "                           'gold_label': 'entailment'}\n",
    "            i += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we'll grab the adjective-noun pairs and add them to the same dictionary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for sentence in tqdm(sentences):\n",
    "    words = nlp(sentence)\n",
    "    for i in range(len(words) - 1):\n",
    "        word_1, word_2 = words[i:i+2]\n",
    "        if ((word_1.pos_ == 'ADJ') & (word_2.pos_ == 'NOUN')):            \n",
    "            subjects[i] = {'sentence1': word.text,\n",
    "                           'sentence2': sentence,\n",
    "                           'gold_label': 'entailment'}\n",
    "            i += 1\n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll now transform that dictionary into a dataframe so that it can be combined with the ones we loaded in before."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subjects = pd.DataFrame(subjects).T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# the base dataframe\n",
    "Here's the combined dataframe with all four datasets. As usual, pandas makes manipulation of the data at this stage super simple."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.concat([multinli, snli, coco, subjects]).fillna('')\n",
    "df = df.drop(df[df['gold_label'] == '-'].index)\n",
    "df.reset_index(inplace=True, drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# word vectors, vocabulary and text preprocessing\n",
    "For our text to be meaningfully interpretable by the neural network, we'll seed it with their representations as given by fasttext. \n",
    "\n",
    "First we load in the fasttext vectors, and then process all of our sentences so that they are stored as lists of indexes (mapped to their corresponding word vectors), rather than as raw strings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wv_path = '/mnt/efs/nlp/word_vectors/fasttext/wiki-news-300d-1M.vec'\n",
    "wv_file = io.open(wv_path, 'r', encoding='utf-8', newline='\\n', errors='ignore')\n",
    "\n",
    "fasttext = {line.split()[0]: np.array(line.split()[1:])\n",
    "            for line in tqdm(list(wv_file))}\n",
    "\n",
    "pad_value, start_value, end_value = 0.25, 0.5, 0.75\n",
    "fasttext['<p>'] = np.full(shape=(300,), fill_value=pad_value)\n",
    "fasttext['<s>'] = np.full(shape=(300,), fill_value=start_value)\n",
    "fasttext['</s>'] = np.full(shape=(300,), fill_value=end_value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(sentence):\n",
    "    index_list = ([word_to_index['<s>']] + \n",
    "                  [word_to_index[w] for w in word_tokenize(sentence) if w in fasttext] + \n",
    "                  [word_to_index['</s>']])\n",
    "    return index_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_to_index = {word: index for index, word in enumerate(list(fasttext.keys()))}\n",
    "index_to_word = {index: word for index, word in enumerate(list(fasttext.keys()))}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "index_to_wordvec = np.zeros((len(fasttext), 300))\n",
    "for word in tqdm(list(fasttext.keys())):\n",
    "    index_to_wordvec[word_to_index[word]] = fasttext[word]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['sentence1'] = df['sentence1'].apply(str.lower)\n",
    "df['sentence2'] = df['sentence2'].apply(str.lower)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['sentence1'] = df['sentence1'].progress_apply(preprocess)\n",
    "df['sentence2'] = df['sentence2'].progress_apply(preprocess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "le = LabelEncoder()\n",
    "df['gold_label'] = le.fit_transform(df['gold_label'].values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# dataset and dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NLIDataset(Dataset):\n",
    "    def __init__(self, dataframe):\n",
    "        self.sentence1s = dataframe['sentence1'].values\n",
    "        self.sentence2s = dataframe['sentence2'].values\n",
    "        self.labels = dataframe['gold_label'].values\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        s1 = self.sentence1s[index]\n",
    "        s2 = self.sentence2s[index]\n",
    "        label = self.labels[index]\n",
    "        return s1, s2, label\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## sort dataset by length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shuffled_df = df.sample(frac=1).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "split_ratio = 0.8\n",
    "train_size = int(split_ratio * len(df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = shuffled_df.loc[:train_size]\n",
    "test_df  = shuffled_df.loc[train_size:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = NLIDataset(train_df)\n",
    "test_dataset = NLIDataset(test_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We've vastly imbalanced the original dataset's classes by adding so many entailments, so we calculate the dataset's class weights to rebalance the training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_weights = (train_df['gold_label']\n",
    "                 .value_counts(normalize=True)\n",
    "                 .sort_index()\n",
    "                 .values)\n",
    "\n",
    "class_weights = torch.Tensor(class_weights).cuda()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# dataloader with custom `collate_fn()`\n",
    "The custom collate function adds padding to the network's inputs at each batch, ensuring that each batch is rectangular. I know that could be done with `pack_padded_sequence()` etc but they're strange beasts without many parallels in other frameworks, and documentation/examples are lacking at the moment, so I'd rather write something myself which I understand rather than mess things up by using something that doesn't make sense to me yet. In all other ways, this dataloader is the same as the ones we've used in the previous notebooks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sentence_to_indexes(sentence):\n",
    "    tokenised = word_tokenize(sentence)\n",
    "    indexes = [word_to_index[word] \n",
    "               for word in tokenised \n",
    "               if word in word_to_index]\n",
    "    return indexes\n",
    "\n",
    "def pad_sequence(sentences, pad_length=None):\n",
    "    if pad_length is None:\n",
    "        pad_length = max([len(sent) for sent in sentences])\n",
    "\n",
    "    padded = np.full((len(sentences), pad_length), word_to_index['<p>'])\n",
    "    for i, sentence in enumerate(sentences):\n",
    "        padded[i][pad_length - len(sentence):] = sentence\n",
    "    return padded\n",
    "\n",
    "\n",
    "def custom_collate_fn(batch):\n",
    "    s1, s2, labels = zip(*batch)\n",
    "    \n",
    "    batch_size = len(labels)\n",
    "    seq_length = max([len(s) for s in (s1 + s2)])\n",
    "\n",
    "    padded_s1 = pad_sequence(s1, pad_length=seq_length)\n",
    "    padded_s2 = pad_sequence(s2, pad_length=seq_length)\n",
    "    \n",
    "    wv_s1 = np.stack([[index_to_wordvec[i] for i in seq] for seq in padded_s1])\n",
    "    wv_s2 = np.stack([[index_to_wordvec[i] for i in seq] for seq in padded_s2])\n",
    "    \n",
    "    return wv_s1, wv_s2, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 64\n",
    "\n",
    "train_loader = DataLoader(dataset=train_dataset,\n",
    "                          batch_size=batch_size,\n",
    "                          num_workers=5,\n",
    "                          shuffle=True,\n",
    "                          collate_fn=custom_collate_fn)\n",
    "\n",
    "test_loader = DataLoader(dataset=test_dataset,\n",
    "                         batch_size=batch_size,\n",
    "                         num_workers=5,\n",
    "                         collate_fn=custom_collate_fn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# build models\n",
    "We're replicating InferSent's architecture with a 1-layer, 2048-dimensional, biderectional LSTM providing the brains of the network, followed by a simple compression down to the 3-dimensional softmax output. The sentence embedding and NLI-task networks are kept separate (with one nested inside the other at train-time) for simplicity's sake later on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hidden_size = 2048\n",
    "\n",
    "class SentenceEncoder(nn.Module):\n",
    "    def __init__(self, ):\n",
    "        super(SentenceEncoder, self).__init__()\n",
    "        self.enc_lstm = nn.LSTM(input_size=300, \n",
    "                                hidden_size=hidden_size, \n",
    "                                num_layers=1,\n",
    "                                bidirectional=True)\n",
    "        \n",
    "    def forward(self, wv_batch):\n",
    "        embedded, _ = self.enc_lstm(wv_batch)\n",
    "        max_pooled = torch.max(embedded, 1)[0] \n",
    "        return max_pooled\n",
    "\n",
    "\n",
    "class NLINet(nn.Module):\n",
    "    def __init__(self, index_to_wordvec):\n",
    "        super(NLINet, self).__init__()\n",
    "        self.index_to_wordvec = index_to_wordvec\n",
    "        self.encoder = SentenceEncoder()\n",
    "        self.classifier = nn.Sequential(nn.Dropout(0.2),\n",
    "                                        nn.Linear(hidden_size*8, 128),\n",
    "                                        nn.ReLU(),\n",
    "                                        nn.Dropout(0.2),\n",
    "                                        nn.Linear(128, 3),\n",
    "                                       )\n",
    "\n",
    "    def forward(self, s1, s2):\n",
    "        u, v = self.encoder(s1), self.encoder(s2)\n",
    "        features = torch.cat((u, v, torch.abs(u - v), u * v), 1)\n",
    "        return self.classifier(features)\n",
    "\n",
    "    def encode(self, sentences):\n",
    "        indexes = sentence_to_indexes(sentences)\n",
    "        wvs = torch.Tensor(np.stack([self.index_to_wordvec[i] for i in indexes]))\n",
    "        return self.encoder([wvs])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# train loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "losses = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, train_loader, loss_function, optimiser, n_epochs):\n",
    "    model.train()\n",
    "    for epoch in range(n_epochs):\n",
    "        loop = tqdm(train_loader)\n",
    "        for s1, s2, target in loop:\n",
    "            s1 = torch.FloatTensor(s1).cuda(non_blocking=True)\n",
    "            s2 = torch.FloatTensor(s2).cuda(non_blocking=True)\n",
    "            target = torch.LongTensor(target).cuda(non_blocking=True)\n",
    "\n",
    "            optimiser.zero_grad()\n",
    "            preds = model(s1, s2)\n",
    "\n",
    "            loss = loss_function(preds, target)\n",
    "            loss.backward()\n",
    "            optimiser.step()\n",
    "\n",
    "            n_correct = target.eq(preds.max(1)[1]).cpu().sum()\n",
    "            accuracy = (n_correct / batch_size) * 100\n",
    "\n",
    "            loop.set_description('Epoch {}/{}'.format(epoch + 1, n_epochs))\n",
    "            loop.set_postfix(loss=loss.item(), acc=accuracy.item())\n",
    "            losses.append([loss.item(), accuracy.item()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.backends.cudnn.benchmark = True\n",
    "\n",
    "model = NLINet(index_to_wordvec).to(device)\n",
    "model.load_state_dict(torch.load('/mnt/efs/models/nlinet-2018-10-08.pt'))\n",
    "\n",
    "trainable_parameters = filter(lambda p: p.requires_grad, model.parameters())\n",
    "optimiser = optim.Adam(trainable_parameters, lr=0.001)\n",
    "loss_function = nn.CrossEntropyLoss(weight=class_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train(model=model,\n",
    "      train_loader=train_loader,\n",
    "      loss_function=loss_function,\n",
    "      optimiser=optimiser,\n",
    "      n_epochs=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_data = pd.Series(losses).rolling(window=50).mean()\n",
    "ax = loss_data.plot();\n",
    "ax.set_xlim(0,);\n",
    "ax.set_ylim(0, 1.1);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), '/mnt/efs/models/nlinet-2018-10-08.pt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# evaluate\n",
    "we can inspect and evaluate the model by having a direct look at the similarity of a few query sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def embed(sentence):\n",
    "    indexes = ([word_to_index['<s>']] + \n",
    "               sentence_to_indexes(sentence) +\n",
    "               [word_to_index['</s>']])\n",
    "    wvs = np.stack([index_to_wordvec[i] for i in indexes])\n",
    "    embedding = model.encoder(torch.Tensor([wvs]).cuda()).cpu().data.numpy()\n",
    "    return embedding.squeeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = (pd.concat([multinli, snli, coco])\n",
    "             .fillna('')\n",
    "             ['sentence1']\n",
    "             .sample(20)\n",
    "             .values)\n",
    "\n",
    "embeddings = [embed(sentence) for sentence in sentences]\n",
    "\n",
    "for i, sentence in enumerate(sentences):\n",
    "    print(i, sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.spatial.distance import cdist\n",
    "distance_matrix = cdist(embeddings, embeddings, metric='cosine')\n",
    "sns.heatmap(distance_matrix);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "it's obviously important to check that we haven't begun overfitting by comparing our knowledge of the train set to the performance on the test set. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_losses = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    loop = tqdm(test_loader)\n",
    "    for s1, s2, target in loop:\n",
    "        s1 = torch.FloatTensor(s1).cuda(non_blocking=True)\n",
    "        s2 = torch.FloatTensor(s2).cuda(non_blocking=True)\n",
    "        target = torch.LongTensor(target).cuda(non_blocking=True)\n",
    "\n",
    "        optimiser.zero_grad()\n",
    "        preds = model.eval()(s1, s2)\n",
    "\n",
    "        test_loss = loss_function(preds, target)\n",
    "        loop.set_postfix(loss=test_loss.item())\n",
    "        test_losses.append(test_loss.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(np.mean(test_losses))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# test on wellcome titles\n",
    "We can also have a look at how our model does on the tiles of works in the wellcome collection catalogue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "meta = pd.read_json('/mnt/efs/other/works.json', lines=True)\n",
    "meta.index = meta['identifiers'].apply(lambda x: x[0]['value']).rename()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "titles = meta['title'].values\n",
    "title_embeddings = np.array([embed(sentence) for sentence in tqdm(titles)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_sentence = 'table'\n",
    "query_embedding = embed(query_sentence).reshape(1, -1)\n",
    "distances = cdist(query_embedding, title_embeddings, metric='cosine')\n",
    "print(query_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "titles[np.argsort(distances)][0][:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# save model\n",
    "We'll continue to use the model we've trained here, so let's save all the necessary files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "sentence_encoder = model.encoder\n",
    "torch.save(sentence_encoder.state_dict(), '/mnt/efs/models/sentence-encoder-2018-10-08.pt')\n",
    "\n",
    "np.save('/mnt/efs/models/index_to_wordvec.npy', index_to_wordvec)\n",
    "pickle.dump(word_to_index, open('/mnt/efs/models/word_to_index.pkl', 'wb'))\n",
    "pickle.dump(index_to_word, open('/mnt/efs/models/index_to_word.pkl', 'wb'))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
