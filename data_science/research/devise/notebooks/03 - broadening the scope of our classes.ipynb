{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# broadening the scope of our classes\n",
    "So far, we've used ImageNet data as the basis for teaching our machine about the relationship between language and visual features. The 200 classes of tiny ImageNet and the 1000 classes of ImageNet are aggregations of images described by a number of WordNet nouns - that's where all of our `wordnet_id`s come from.  \n",
    "In this notebook, we test the hypothesis that we needn't confine ourselves to the 1000 classes of ImageNet. Instead of a large number of images associated with a small number of classes, we can invert the relationship to obtain a small number of images for each of a large number of classes, thereby mapping the word-vector space more completely. When using ImageNet, we precisely define the points in word vector space which map to certain visual features, but the rest of the space must be geometrically inferred.  By reducing the precision but increasing the breadth, the hope is that we'll see a more informed network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "sns.set_style('whitegrid')\n",
    "plt.rcParams['figure.figsize'] = (20, 20)\n",
    "\n",
    "import os\n",
    "import io\n",
    "import requests\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "from scipy.spatial.distance import cdist\n",
    "from scipy.io import loadmat\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "import torch\n",
    "from torch import nn, optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import models, transforms\n",
    "\n",
    "from tqdm._tqdm_notebook import tqdm_notebook as tqdm\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# get wordnet nouns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "id_to_word = {}\n",
    "wordnet_url = 'http://files.fast.ai/data/classids.txt'\n",
    "\n",
    "for line in requests.get(wordnet_url).text.split('\\n'):\n",
    "    try:\n",
    "        id, word = line.split()\n",
    "        id_to_word[id] = word\n",
    "    except: pass    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wordnet_nouns = [word.lower().replace('_', '-') for word in id_to_word.values()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# load word vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wv_path = '/mnt/efs/nlp/word_vectors/fasttext/crawl-300d-2M.vec'\n",
    "wv_file = io.open(wv_path, 'r', encoding='utf-8', newline='\\n', errors='ignore')\n",
    "\n",
    "word_vectors = {line.split()[0]: np.array(line.split()[1:]).astype(np.float)\n",
    "                for line in tqdm(list(wv_file))}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_vector_set = set(word_vectors.keys())\n",
    "wordnet_set = set(wordnet_nouns)\n",
    "\n",
    "valid_queries = list(word_vector_set.intersection(wordnet_set))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# get images of the valid wordnet nouns from google\n",
    "We'll use google images to obtain the corresponding image sets for our wordnet nouns. By concatenating the wordnet noun with a google search query string and parsing the response with beautifulsoup, we can build up a broad set of small images relatively quickly, as demonstrated below with a random query."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = np.random.choice(valid_queries)\n",
    "base_url = 'https://www.google.com/search?tbm=isch&q='\n",
    "\n",
    "soup = BeautifulSoup(requests.get(base_url + query).content)\n",
    "urls = [img['src'] for img in soup.findAll('img')]\n",
    "\n",
    "print(query)\n",
    "\n",
    "images = [(Image.open(io.BytesIO(requests.get(url).content))\n",
    "           .resize((64, 64), resample=Image.BILINEAR)\n",
    "           .convert('RGB'))\n",
    "          for url in urls]\n",
    "\n",
    "Image.fromarray(np.concatenate(images, axis=1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can wrap up that functionality for convenience"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def image_search(query):\n",
    "    base_url = 'https://www.google.com/search?tbm=isch&q='\n",
    "\n",
    "    soup = BeautifulSoup(requests.get(base_url + query).content)\n",
    "    urls = [img['src'] for img in soup.findAll('img')]\n",
    "    \n",
    "    images = [Image.open(io.BytesIO(requests.get(url).content)).convert('RGB')\n",
    "              for url in urls]\n",
    "    \n",
    "    return images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "images = [i.resize((224,224)) for i in image_search('dog')]\n",
    "Image.fromarray(np.concatenate(images, axis=1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# save the data\n",
    "Let's churn through our wordnet nouns and save a collection of images for each"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_path = '/mnt/efs/images/google_scraping/'\n",
    "\n",
    "for query in tqdm(np.random.choice(valid_queries, 2000)):\n",
    "    images = image_search(query)\n",
    "    for i, image in enumerate(images):\n",
    "        image.save(save_path + '{}_{}.jpg'.format(query, i))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "from here onwards, the process is much the same as before. We'll define our data loading processes, build a simple model with a pre-trained feature-extracting backbone and train it until the loss bottoms out. Then we'll evaluate how well it has generalised against a pre-defined test set and run some test queries using out-of-vocabulary words.\n",
    "\n",
    "# datasets and dataloaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = {}\n",
    "\n",
    "for file_name in os.listdir(save_path):\n",
    "    df[save_path + file_name] = file_name.split('_')[0]\n",
    "\n",
    "df = pd.Series(df).to_frame().reset_index()\n",
    "df.columns = ['path', 'word']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.sample(frac=1).reset_index(drop=True) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "split_ratio = 0.8\n",
    "train_size = int(split_ratio * len(df))\n",
    "\n",
    "train_df = df.loc[:train_size]\n",
    "test_df  = df.loc[train_size:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ImageDataset(Dataset):\n",
    "    def __init__(self, dataframe, word_vectors,\n",
    "                 transform=transforms.ToTensor()):\n",
    "        self.image_paths = dataframe['path'].values\n",
    "        self.words = dataframe['word'].values\n",
    "        self.word_vectors = word_vectors\n",
    "        self.transform = transform\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        image = Image.open(self.image_paths[index]).convert('RGB')\n",
    "        if self.transform is not None:\n",
    "            image = self.transform(image)\n",
    "\n",
    "        target = torch.Tensor(word_vectors[self.words[index]])\n",
    "        return image, target\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_transform = transforms.Compose([transforms.RandomResizedCrop(224, scale=[0.6, 0.9]),\n",
    "                                      transforms.RandomHorizontalFlip(),\n",
    "                                      transforms.RandomRotation(15),\n",
    "                                      transforms.RandomGrayscale(0.25),\n",
    "                                      transforms.ToTensor()])\n",
    "\n",
    "test_transform = transforms.Compose([transforms.RandomResizedCrop(224, scale=[0.6, 0.9]),\n",
    "                                     transforms.ToTensor()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = ImageDataset(train_df, word_vectors, train_transform)\n",
    "test_dataset = ImageDataset(test_df, word_vectors, test_transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 128\n",
    "\n",
    "train_loader = DataLoader(dataset=train_dataset,\n",
    "                          batch_size=batch_size,\n",
    "                          num_workers=5,\n",
    "                          shuffle=True)\n",
    "\n",
    "test_loader = DataLoader(dataset=test_dataset,\n",
    "                         batch_size=batch_size,\n",
    "                         num_workers=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# building the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "backbone = models.vgg16_bn(pretrained=True).features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for param in backbone.parameters():\n",
    "    param.requires_grad = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DeViSE(nn.Module):\n",
    "    def __init__(self, backbone, target_size=300):\n",
    "        super(DeViSE, self).__init__()\n",
    "        self.backbone = backbone\n",
    "        self.head = nn.Sequential(\n",
    "            nn.Linear(in_features=(25088), out_features=target_size*2),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Dropout(),\n",
    "            nn.Linear(in_features=target_size*2, out_features=target_size),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Dropout(),\n",
    "            nn.Linear(in_features=target_size, out_features=target_size),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.backbone(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.head(x)\n",
    "        x = x / x.max()\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "devise_model = DeViSE(backbone).to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# train loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "losses = []\n",
    "\n",
    "def train(model, train_loader, loss_function, optimiser, n_epochs):\n",
    "    for epoch in range(n_epochs):\n",
    "        model.train()\n",
    "        loop = tqdm(train_loader)\n",
    "        for images, targets in loop:\n",
    "            images = images.cuda(non_blocking=True)\n",
    "            targets = targets.cuda(non_blocking=True)\n",
    "            flags = torch.ones(len(targets)).cuda(non_blocking=True)\n",
    "\n",
    "            optimiser.zero_grad()\n",
    "            predictions = model(images)\n",
    "\n",
    "            loss = loss_function(predictions, targets, flags)\n",
    "            loss.backward()\n",
    "            optimiser.step()\n",
    "\n",
    "            loop.set_description('Epoch {}/{}'.format(epoch + 1, n_epochs))\n",
    "            loop.set_postfix(loss=loss.item())\n",
    "            losses.append(loss.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainable_parameters = filter(lambda p: p.requires_grad, devise_model.parameters())\n",
    "\n",
    "loss_function = nn.CosineEmbeddingLoss()\n",
    "optimiser = optim.Adam(trainable_parameters, lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train(model=devise_model,\n",
    "      n_epochs=3,\n",
    "      train_loader=train_loader,\n",
    "      loss_function=loss_function,\n",
    "      optimiser=optimiser)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_data = pd.Series(losses).rolling(window=15).mean()\n",
    "ax = loss_data.plot();\n",
    "\n",
    "ax.set_xlim(0,);\n",
    "ax.set_ylim(0, 1);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# evaluate on test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = []\n",
    "test_loss = []\n",
    "\n",
    "devise_model.eval()\n",
    "with torch.no_grad():\n",
    "    test_loop = tqdm(test_loader)\n",
    "    for images, targets in test_loop:\n",
    "        images = images.cuda(non_blocking=True)\n",
    "        targets = targets.cuda(non_blocking=True)\n",
    "        flags = torch.ones(len(targets)).cuda(non_blocking=True)\n",
    "        \n",
    "        predictions = devise_model(images)\n",
    "        loss = loss_function(predictions, targets, flags)\n",
    "\n",
    "        preds.append(predictions.cpu().data.numpy())\n",
    "        test_loss.append(loss.item())\n",
    "\n",
    "        test_loop.set_description('Test set')\n",
    "        test_loop.set_postfix(loss=np.mean(test_loss[-5:]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = np.concatenate(preds).reshape(-1, 300)\n",
    "np.mean(test_loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# run a search on the predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def search(query, n=5):\n",
    "    image_paths = test_df['path'].values\n",
    "    distances = cdist(word_vectors[query].reshape(1, -1), preds)\n",
    "    closest_n_paths = image_paths[np.argsort(distances)].squeeze()[:n]\n",
    "    close_images = [np.array(Image.open(image_path).convert('RGB').resize((224,224)))\n",
    "                    for image_path in closest_n_paths]\n",
    "    return Image.fromarray(np.concatenate(close_images, axis=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "search('bridge')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "again, this works! We're getting somewhere now, and making significant changes to the established theory set out in the original DeViSE paper."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
