{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# scaling up to imagenet\n",
    "We've seen how effective DeViSE can be on a small set of data, but it's equally good when it's applied to a large dataset. Here I'll step through the exact same process but with a more complete imagenet dump of imagenet: the [ImageNet Large Scale Visual Recognition Challenge 2014 (ILSVRC2014)](http://image-net.org/challenges/LSVRC/2014/download-images-5jj5.php). \n",
    "\n",
    "We'll only use the validation dataset here, which alone is already more than 6GB. The test set is almost 140GB, which feels like overkill given the already good performance we achieved on tiny imagenet. Again, the fact that we're not entering the competition frees us up to use their data in a way which is appropriate for us - splitting the original competition's validation data into a new train and test set is perfectly valid."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "sns.set_style('whitegrid')\n",
    "plt.rcParams['figure.figsize'] = (20, 20)\n",
    "\n",
    "import os\n",
    "import io\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "from scipy.spatial.distance import cdist\n",
    "from scipy.io import loadmat\n",
    "\n",
    "import torch\n",
    "from torch import nn, optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import models, transforms\n",
    "\n",
    "from tqdm._tqdm_notebook import tqdm_notebook as tqdm\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# wordvectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wv_path = '/mnt/efs/nlp/word_vectors/fasttext/crawl-300d-2M.vec'\n",
    "wv_file = io.open(wv_path, 'r', encoding='utf-8', newline='\\n', errors='ignore')\n",
    "\n",
    "fasttext = {line.split()[0]: np.array(line.split()[1:]).astype(np.float)\n",
    "            for line in tqdm(list(wv_file)[1:])}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_wv = np.array(list(fasttext.values())).mean(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocabulary = set(fasttext.keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# wordnet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean = lambda x: x.lower().strip().split(', ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mat = loadmat('/home/jupyter/ILSVRC2012_devkit_t12/data/meta.mat')\n",
    "wnid_to_words = {line[0][1][0]: clean(line[0][2][0]) for line in mat['synsets']}\n",
    "competition_id_to_wnid = {line[0][0][0][0]: line[0][1][0] for line in mat['synsets']}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wnid_to_wordvector = {wnid: (np.array([fasttext[word] \n",
    "                                       if word in fasttext \n",
    "                                       else mean_wv \n",
    "                                       for word in words ])\n",
    "                             .mean(axis=0))\n",
    "                      for wnid, words in wnid_to_words.items()}\n",
    "\n",
    "wnids = list(wnid_to_wordvector.keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# example data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "id_path = '/mnt/efs/images/ILSVRC2012_validation_ground_truth.txt'\n",
    "competition_ids = pd.read_csv(id_path, header=None).values.squeeze()\n",
    "\n",
    "image_path = '/mnt/efs/images/ILSVRC2012/'\n",
    "image_paths = np.sort([image_path + file_name for file_name in os.listdir(image_path)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "index = np.random.choice(50000)\n",
    "competition_id = competition_ids[index]\n",
    "wnid = competition_id_to_wnid[competition_id]\n",
    "\n",
    "print(' '.join(wnid_to_words[wnid]))\n",
    "Image.open(image_paths[index])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# datasets and dataloaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = dict(zip(image_paths, competition_ids))\n",
    "\n",
    "df = pd.Series(df).to_frame().reset_index()\n",
    "df.columns = ['path', 'wnid']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.sample(frac=1).reset_index(drop=True) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "split_ratio = 0.8\n",
    "train_size = int(split_ratio * len(df))\n",
    "\n",
    "train_df = df.loc[:train_size]\n",
    "test_df  = df.loc[train_size:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ImageDataset(Dataset):\n",
    "    def __init__(self, dataframe, \n",
    "                 competition_id_to_wnid, wnid_to_wordvector,\n",
    "                 transform):\n",
    "        self.image_paths = dataframe['path'].values\n",
    "        self.wnids = dataframe['wnid'].values\n",
    "        self.competition_id_to_wnid = competition_id_to_wnid\n",
    "        self.wnid_to_wordvector = wnid_to_wordvector\n",
    "        self.transform = transform\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        image = Image.open(self.image_paths[index]).convert('RGB')\n",
    "        if self.transform is not None:\n",
    "            image = self.transform(image)\n",
    "\n",
    "        wnid = competition_id_to_wnid[self.wnids[index]]\n",
    "        target = torch.Tensor(wnid_to_wordvector[wnid])\n",
    "        return image, target\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.wnids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_transform = transforms.Compose([transforms.RandomResizedCrop(224, scale=[0.5, 0.9]),\n",
    "                                      transforms.RandomHorizontalFlip(),\n",
    "                                      transforms.RandomRotation(15),\n",
    "                                      transforms.RandomGrayscale(0.25),\n",
    "                                      transforms.ToTensor()])\n",
    "\n",
    "test_transform = transforms.Compose([transforms.RandomResizedCrop(224, scale=[0.5, 0.9]),\n",
    "                                     transforms.ToTensor()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = ImageDataset(dataframe=train_df, \n",
    "                             competition_id_to_wnid=competition_id_to_wnid, \n",
    "                             wnid_to_wordvector=wnid_to_wordvector, \n",
    "                             transform=train_transform)\n",
    "\n",
    "test_dataset = ImageDataset(dataframe=test_df, \n",
    "                            competition_id_to_wnid=competition_id_to_wnid, \n",
    "                            wnid_to_wordvector=wnid_to_wordvector, \n",
    "                            transform=test_transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 128\n",
    "\n",
    "train_loader = DataLoader(dataset=train_dataset,\n",
    "                          batch_size=batch_size,\n",
    "                          num_workers=5,\n",
    "                          shuffle=True)\n",
    "\n",
    "test_loader = DataLoader(dataset=test_dataset,\n",
    "                         batch_size=batch_size,\n",
    "                         num_workers=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# building the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "backbone = models.vgg16_bn(pretrained=True).features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for param in backbone.parameters():\n",
    "    param.requires_grad = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DeViSE(nn.Module):\n",
    "    def __init__(self, backbone, target_size=300):\n",
    "        super(DeViSE, self).__init__()\n",
    "        self.backbone = backbone\n",
    "        self.head = nn.Sequential(\n",
    "            nn.Linear(in_features=(25088), out_features=target_size*2),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Dropout(),\n",
    "            nn.Linear(in_features=target_size*2, out_features=target_size),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Dropout(),\n",
    "            nn.Linear(in_features=target_size, out_features=target_size),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.backbone(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.head(x)\n",
    "        x = x / x.max()\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "devise_model = DeViSE(backbone, target_size=300).to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# train loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "losses = []\n",
    "\n",
    "def train(model, train_loader, loss_function, optimiser, n_epochs):\n",
    "    for epoch in range(n_epochs):\n",
    "        model.train()\n",
    "        loop = tqdm(train_loader)\n",
    "        for images, targets in loop:\n",
    "            images = images.cuda(non_blocking=True)\n",
    "            targets = targets.cuda(non_blocking=True)\n",
    "            flags = torch.ones(len(targets)).cuda(non_blocking=True)\n",
    "            \n",
    "            optimiser.zero_grad()\n",
    "            predictions = model(images)\n",
    "\n",
    "            loss = loss_function(predictions, targets, flags)\n",
    "            loss.backward()\n",
    "            optimiser.step()\n",
    "\n",
    "            loop.set_description('Epoch {}/{}'.format(epoch + 1, n_epochs))\n",
    "            loop.set_postfix(loss=loss.item())\n",
    "            losses.append(loss.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainable_parameters = filter(lambda p: p.requires_grad, devise_model.parameters())\n",
    "\n",
    "loss_function = nn.CosineEmbeddingLoss()\n",
    "optimiser = optim.Adam(trainable_parameters, lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train(model=devise_model,\n",
    "      n_epochs=3,\n",
    "      train_loader=train_loader,\n",
    "      loss_function=loss_function,\n",
    "      optimiser=optimiser)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_data = pd.Series(losses).rolling(window=15).mean()\n",
    "ax = loss_data.plot();\n",
    "\n",
    "ax.set_xlim(0,);\n",
    "ax.set_ylim(0, 1);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# evaluate on test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = []\n",
    "test_loss = []\n",
    "\n",
    "devise_model.eval()\n",
    "with torch.no_grad():\n",
    "    test_loop = tqdm(test_loader)\n",
    "    for images, targets in test_loop:\n",
    "        images = images.cuda(non_blocking=True)\n",
    "        targets = targets.cuda(non_blocking=True)\n",
    "        flags = torch.ones(len(targets)).cuda(non_blocking=True)\n",
    "\n",
    "        predictions = devise_model(images)\n",
    "        loss = loss_function(predictions, targets, flags)\n",
    "\n",
    "        preds.append(predictions.cpu().data.numpy())\n",
    "        test_loss.append(loss.item())\n",
    "\n",
    "        test_loop.set_description('Test set')\n",
    "        test_loop.set_postfix(loss=np.mean(test_loss[-5:]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = np.concatenate(preds).reshape(-1, 300)\n",
    "np.mean(test_loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# run a search on the predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def search(query, n=5):\n",
    "    image_paths = test_df['path'].values\n",
    "    distances = cdist(fasttext[query].reshape(1, -1), preds)\n",
    "    closest_n_paths = image_paths[np.argsort(distances)].squeeze()[:n]\n",
    "    close_images = [np.array(Image.open(image_path).resize((224, 224)).convert('RGB')) \n",
    "                    for image_path in closest_n_paths]\n",
    "    return Image.fromarray(np.concatenate(close_images, axis=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "search('bridge')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Great, that works too. We're now working with much larger, more complex data but the network is still able to make inferences about the interactions between written and visual language."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
