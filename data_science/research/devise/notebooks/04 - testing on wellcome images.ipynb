{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# testing on wellcome images\n",
    "We can now test our models' performance when transferred onto the Wellcome images dataset. In doing so, we'll get a better understanding of how well they generalise and which gaps in their knowledge we'll need to fill as we continue to modify them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "sns.set_style('whitegrid')\n",
    "plt.rcParams['figure.figsize'] = (20, 20)\n",
    "\n",
    "import os\n",
    "import io\n",
    "import requests\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "from scipy.spatial.distance import cdist\n",
    "from scipy.io import loadmat\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "import torch\n",
    "from torch import nn, optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import models, transforms\n",
    "\n",
    "from tqdm._tqdm_notebook import tqdm_notebook as tqdm\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# load word vectors\n",
    "Although we can get rid of our wordnet files in this inference-only section, we still need to create the word-vector space in which we'll perform searches. This will allow us to search for words which didn't appear in the original training set of word-photo pairs. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wv_path = '/mnt/efs/nlp/word_vectors/fasttext/crawl-300d-2M.vec'\n",
    "wv_file = io.open(wv_path, 'r', encoding='utf-8', newline='\\n', errors='ignore')\n",
    "\n",
    "word_vectors = {line.split()[0]: np.array(line.split()[1:]).astype(np.float)\n",
    "                for line in tqdm(list(wv_file))}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# model\n",
    "We'll now build our model, with the exact same configuration as in the last few notebooks. That model will then be initialised with random weights before injecting the pre-trained weights from the previous notebooks. As we've used the same model throughout, all we need to do is modify the path to the `.pt` weights file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "backbone = models.vgg16_bn(pretrained=True).features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for param in backbone.parameters():\n",
    "    param.requires_grad = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DeViSE(nn.Module):\n",
    "    def __init__(self, backbone, target_size=300):\n",
    "        super(DeViSE, self).__init__()\n",
    "        self.backbone = backbone\n",
    "        self.head = nn.Sequential(\n",
    "            nn.Linear(in_features=(25088), out_features=target_size*2),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Dropout(),\n",
    "            nn.Linear(in_features=target_size*2, out_features=target_size),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Dropout(),\n",
    "            nn.Linear(in_features=target_size, out_features=target_size),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.backbone(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.head(x)\n",
    "        x = x / x.max()\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "devise_model = DeViSE(backbone).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "devise_model.load_state_dict(torch.load('/mnt/efs/models/devise-google-2018-10-03.pt'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# wellcome images dataset and dataloader\n",
    "We can now define the process for ingesting wellcome images. This is a much simpler process than in previous notebooks as we no longer need to asign them a target word. In this section we are only _inferring_ the word-vector positions according to our DeViSE network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = {}\n",
    "\n",
    "for subdir in os.listdir('/mnt/efs/images/wellcome_images/'):\n",
    "    subdir_path = '/mnt/efs/images/wellcome_images/{}/'.format(subdir)\n",
    "    for file_name in os.listdir(subdir_path):\n",
    "        df[subdir_path + file_name] = subdir\n",
    "\n",
    "df = pd.Series(df).to_frame().reset_index()\n",
    "df.columns = ['path', 'word']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.sample(frac=1).reset_index(drop=True) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ImageDataset(Dataset):\n",
    "    def __init__(self, dataframe, transform=transforms.ToTensor()):\n",
    "        self.image_paths = dataframe['path'].values\n",
    "        self.transform = transform\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        image = Image.open(self.image_paths[index]).convert('RGB')\n",
    "        if self.transform is not None:\n",
    "            image = self.transform(image)\n",
    "        return image\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_paths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose([transforms.RandomResizedCrop(224, scale=[0.6, 0.9]),\n",
    "                                transforms.ToTensor()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = ImageDataset(df, transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_loader = DataLoader(dataset=dataset,\n",
    "                         batch_size=128,\n",
    "                         num_workers=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# make predictions for wellcome images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = []\n",
    "\n",
    "devise_model.eval()\n",
    "with torch.no_grad():\n",
    "    test_loop = tqdm(test_loader)\n",
    "    for images in test_loop:\n",
    "        images = images.cuda(non_blocking=True)\n",
    "        predictions = devise_model(images)\n",
    "        preds.append(predictions.cpu().data.numpy())\n",
    "        \n",
    "        test_loop.set_description('Test set')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = np.concatenate(preds).reshape(-1, 300)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# run a search on the predictions\n",
    "We're now ready to run a search in the new, shared space of words and images. Note that the shared space has the exact same geometry as the word-vector space but is now populated by word vector predictions of images. In other words, we've _projected_ our images onto the manifold in 300-dimensional space which is occupied by word-vectors, with the hope that we've preserved their visual-semantic meanings. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def search(query, n=5):\n",
    "    image_paths = df['path'].values\n",
    "    distances = cdist(word_vectors[query].reshape(1, -1), preds)\n",
    "    closest_n_paths = image_paths[np.argsort(distances)].squeeze()[:n]\n",
    "    close_images = [np.array(Image.open(image_path).convert('RGB').resize((224,224)))\n",
    "                    for image_path in closest_n_paths]\n",
    "    return Image.fromarray(np.concatenate(close_images, axis=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "search('sad')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That works reasonably well, but the differences in the style of the datasets is clear. We'll need to address this later."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
