{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "sns.set_style('whitegrid')\n",
    "plt.rcParams['figure.figsize'] = (20, 14)\n",
    "\n",
    "import requests\n",
    "import string\n",
    "\n",
    "import os\n",
    "import json\n",
    "import nltk\n",
    "import spacy\n",
    "import itertools\n",
    "import numpy as np \n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "from scipy.spatial.distance import cdist\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "from tqdm import tqdm as tqdm_\n",
    "tqdm_.pandas()\n",
    "\n",
    "import io\n",
    "from nltk.tokenize import word_tokenize\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "import torch\n",
    "from torch import nn, optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import models, transforms\n",
    "\n",
    "nlp = spacy.load('en')\n",
    "nltk.download('punkt')\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# very simple character level RNN\n",
    "### get a text from gutenberg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "texts = [requests.get('http://www.gutenberg.org/files/{}/{}.txt'.format(i, i)).text\n",
    "         for i in np.random.choice(np.arange(start=1000, stop=1200), 10)]\n",
    "\n",
    "text = '\\n'.join(texts)\n",
    "len(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### define how to chunk and vectorize text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_characters = list(set(text))\n",
    "label_encoder = LabelEncoder().fit(all_characters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_chunk(text, chunk_length=100):\n",
    "    start_index = np.random.randint(0, len(text) - chunk_length - 1)\n",
    "    end_index = start_index + chunk_length\n",
    "    input_text = text[start_index : end_index]\n",
    "    target_character = text[end_index]\n",
    "    return list(input_text), target_character"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### dataset and dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ChunkDataset(Dataset):\n",
    "    def __init__(self, text, label_encoder, length):\n",
    "        self.text = text\n",
    "        self.label_encoder = label_encoder\n",
    "        self.length = length\n",
    "        \n",
    "    def __getitem__(self, index):\n",
    "        input_text, target_character = random_chunk(self.text)\n",
    "        \n",
    "        input_indexes = self.label_encoder.transform(input_text)\n",
    "        target_index = self.label_encoder.transform([target_character])\n",
    "        return input_indexes, target_index\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = ChunkDataset(text, label_encoder, length=80000)\n",
    "test_dataset = ChunkDataset(text, label_encoder, length=2000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 200\n",
    "\n",
    "train_loader = DataLoader(dataset=train_dataset, \n",
    "                          batch_size=batch_size, \n",
    "                          num_workers=5)\n",
    "\n",
    "test_loader = DataLoader(dataset=test_dataset, \n",
    "                         batch_size=batch_size, \n",
    "                         num_workers=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SequenceEncoder(nn.Module):\n",
    "    def __init__(self, num_embeddings, embedding_size, hidden_size, label_encoder):\n",
    "        super(SequenceEncoder, self).__init__()\n",
    "        self.label_encoder = label_encoder\n",
    "        self.embedding = nn.Embedding(num_embeddings, embedding_size)\n",
    "        self.lstm = nn.LSTM(embedding_size, hidden_size)\n",
    "        self.decoder = nn.Sequential(nn.Dropout(0.2),\n",
    "                                     nn.Linear(hidden_size, num_embeddings))\n",
    "\n",
    "    def forward(self, indexes):\n",
    "        embedded = self.embedding(indexes)\n",
    "        output, hidden = self.lstm(embedded)\n",
    "        return self.decoder(output[:, -1])\n",
    "    \n",
    "    def predict_next_character(self, indexes):\n",
    "        preds = self.forward(indexes)[-1]\n",
    "        guess_index = preds.argmax()[0]\n",
    "        next_character = self.label_encoder.inverse_transform([guess_index])\n",
    "        return next_character[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "losses = []\n",
    "def train(model, train_loader, loss_function, optimiser, n_epochs):\n",
    "    for epoch in range(n_epochs):\n",
    "        model.train()\n",
    "        loop = tqdm(train_loader)\n",
    "        for inputs, targets in loop:\n",
    "            inputs = inputs.cuda(non_blocking=True)\n",
    "            targets = targets.cuda(non_blocking=True).view(-1)\n",
    "\n",
    "            optimiser.zero_grad()\n",
    "            preds = model(inputs)\n",
    "\n",
    "            loss = loss_function(preds, targets)\n",
    "            loss.backward()\n",
    "            optimiser.step()\n",
    "            \n",
    "            losses.append(loss.item())\n",
    "            loop.set_description('Epoch {}/{}'.format(epoch + 1, n_epochs))\n",
    "            loop.set_postfix(loss=np.mean(losses[-20:]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.backends.cudnn.benchmark = True\n",
    "\n",
    "model = SequenceEncoder(num_embeddings=len(all_characters), \n",
    "                        embedding_size=128,\n",
    "                        hidden_size=256,\n",
    "                        label_encoder=label_encoder\n",
    "                       ).to(device)\n",
    "\n",
    "trainable_parameters = filter(lambda p: p.requires_grad, model.parameters())\n",
    "optimiser = optim.Adam(trainable_parameters, lr=0.001)\n",
    "loss_function = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train(model=model,\n",
    "      train_loader=train_loader,\n",
    "      loss_function=loss_function,\n",
    "      optimiser=optimiser,\n",
    "      n_epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_data = pd.Series(losses).rolling(window=15).mean()\n",
    "ax = loss_data.plot();\n",
    "ax.set_ylim(0, 5);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_chunk, target_character = random_chunk(text)\n",
    "indexes = label_encoder.transform(text_chunk).reshape(-1,1)\n",
    "indexes = torch.Tensor(indexes).long().cuda()\n",
    "\n",
    "x = model(indexes)[-1]\n",
    "\n",
    "\n",
    "print(''.join(text_chunk))\n",
    "print('''\n",
    "------------------------------\n",
    "predicted character:\\t{}\n",
    "actual character:\\t{}\n",
    "      '''.format(label_encoder.inverse_transform([x.argmax()])[0],\n",
    "                 target_character))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_chunk, target_character = random_chunk(text)\n",
    "\n",
    "def predict(text_chunk):\n",
    "    indexes = label_encoder.transform(text_chunk)\n",
    "    indexes = torch.Tensor(indexes).long().cuda().unsqueeze(0)\n",
    "    return model.predict_next_character(indexes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_chunk, target_character = random_chunk(text)\n",
    "\n",
    "for i in range(500):\n",
    "    text_chunk.append(predict(text_chunk[-20:]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "''.join(text_chunk)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
