{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# inference only demo\n",
    "We're done! We have a working pair of models which produce meaninful shared embeddings for text and images, which we can use to run image searches without relying on detailed metadata. The only thing to do now is ensure that the search process is fast enough to be practical, and lay out all of the pieces we need to run this outside of a notebook environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import pickle\n",
    "import nmslib\n",
    "import urllib\n",
    "import numpy as np \n",
    "\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "nltk.download('punkt')\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# load data\n",
    "First we'll load a bunch of the lookup data we need to make this thing work. Nothing new here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "index_to_wordvec = np.load('/mnt/efs/models/index_to_wordvec.npy')\n",
    "word_to_index = pickle.load(open('/mnt/efs/models/word_to_index.pkl', 'rb'))\n",
    "\n",
    "path_to_id = lambda x: x.split('/')[-1].split('.')[0]\n",
    "image_ids = np.array(list(map(path_to_id, \n",
    "                              np.load('/mnt/efs/models/image_ids.npy'))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# load devise'd embeddings for all images\n",
    "We pre-computed the learned visual-semantic embeddings for all of our images at the end of the last notebook, so we can just reload them here. Remember, they're sentence-space representations of the images, so all that needs to happen at query-time is the embedding of the query sentence into the same space, and a KNN lookup of the most similar images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings = np.load('/mnt/efs/models/embeddings.npy').reshape(-1, 4096)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# utils\n",
    "Again, we'll create a couple of utility functions to shrink the sentence embedding process down to a single function call."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sentence_to_indexes(sentence):\n",
    "    tokenised = word_tokenize(sentence)\n",
    "    indexes = [word_to_index[word] \n",
    "               for word in tokenised \n",
    "               if word in word_to_index]\n",
    "    return indexes\n",
    "\n",
    "def embed(sentence):\n",
    "    indexes = ([word_to_index['<s>']] + \n",
    "               sentence_to_indexes(sentence) +\n",
    "               [word_to_index['</s>']])\n",
    "    wvs = np.stack([index_to_wordvec[i] for i in indexes])\n",
    "    embedding = model(torch.Tensor([wvs]).cuda()).cpu().data.numpy()    \n",
    "    return embedding.squeeze()\n",
    "\n",
    "def embed_paragraph(paragraph):\n",
    "    sentences = sent_tokenize(paragraph)\n",
    "    if len(sentences) == 0:\n",
    "        return None\n",
    "    else:\n",
    "        embeddings = [embed(sentence) for sentence in sentences]\n",
    "        return np.array(embeddings).max(axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# sentence embedding model\n",
    "Now that we're only inferring an embedding for each sentence, we can ignore the `NLINet()` part of the network from notebook 8. We no longer need to classify sentence pairs or backpropagate any weights, so the remaining network is incredibly small and can be run without much trouble on a CPU. We saved the weights for this half of the network at the end of the last notebook, which we can inject into the matching network architecture here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hidden_size = 2048\n",
    "\n",
    "class SentenceEncoder(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SentenceEncoder, self).__init__()\n",
    "        self.enc_lstm = nn.LSTM(input_size=300, \n",
    "                                hidden_size=hidden_size, \n",
    "                                num_layers=1,\n",
    "                                bidirectional=True)\n",
    "        \n",
    "    def forward(self, wv_batch):\n",
    "        embedded, _ = self.enc_lstm(wv_batch)\n",
    "        max_pooled = torch.max(embedded, 1)[0] \n",
    "        return max_pooled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = SentenceEncoder().to(device)\n",
    "model_path = '/mnt/efs/models/sentence-encoder-2018-10-08.pt'\n",
    "\n",
    "model.load_state_dict(torch.load(model_path))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# create nmslib search index\n",
    "In the previous notebooks we've run searches by brute-forcing our way across the dataset, measuring the distance from our query embedding to every other individual point in sentence-space. This is exact, but _super_ inefficient, especially in a high-volume, high-dimensional case like ours. Here, and in our demo app, we'll use an _approximate_-nearest neighbours algorithm which transforms our data in sentence-embedding space into a hierarchical graph/tree structure, allowing us to traverse the whole thing with very few calculations. The approximate-ness of this ANN algorithm is small, and in the end we lose very little information by transforming it into this structure.  \n",
    "Similar libraries like [annoy](https://github.com/spotify/annoy) leverage roughly the same technique to find nearest neighbours in high dimensional space, but [nmslib has been shown to be the most efficient](https://www.benfrederickson.com/approximate-nearest-neighbours-for-recommender-systems/) and we have no reason not to use it here.  \n",
    "Pre-computing the index takes a while, but it vastly reduces the search time when we run a query. The index can also be saved in binary form and reloaded elsewhere, so we don't have to re-run that expensive computation every time we restart our demo. The python bindings for nmslib are very straightforward - we can create our fully functional index in just three lines of code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "index = nmslib.init(method='hnsw', space='cosinesimil')\n",
    "index.addDataPointBatch(embeddings)\n",
    "index.createIndex({'post': 2}, print_progress=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# search\n",
    "Let's run a search, returning the closest MIRO IDs and attaching them to a `/works` query URL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def search(query):\n",
    "    neighbour_indexes, _ = index.knnQuery(embed(query), k=10)\n",
    "    return image_ids[neighbour_indexes]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = search('mri brain scan')\n",
    "base_url = 'https://wellcomecollection.org/works?query='\n",
    "url_query = urllib.parse.quote_plus(' '.join(results))\n",
    "print(base_url + url_query)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That's it - super fast, super effective image search with no metadata necessary! \n",
    "\n",
    "We've turned this notebook into a demo app hosted on AWS, which you can play with [here](http://labs.wellcomecollection.org/devise/index.html)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
