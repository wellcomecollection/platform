{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# recreating the paper with tiny imagenet\n",
    "First we're going to take a stab at the most basic version of DeViSE: learning a mapping between image feature vectors and their corresponding labels' word vectors for imagenet classes. Doing this with the entirety of imagenet feels like overkill, so we'll start with tiny imagenet.\n",
    "\n",
    "## tiny imagenet\n",
    "Tiny imagenet is a subset of imagenet which has been preprocessed for the stanford computer vision course CS231N. It's freely available to download and ideal for putting together quick and easy tests and proof-of-concept work in computer vision. From [their website](https://tiny-imagenet.herokuapp.com/):\n",
    "> Tiny Imagenet has 200 classes. Each class has 500 training images, 50 validation images, and 50 test images.\n",
    "\n",
    "Images are also resized to 64x64px, making the whole dataset small and fast to load. \n",
    "\n",
    "We'll use it to demo the DeViSE idea here. Lets load in a few of the packages we'll use in the project - plotting libraries, numpy, pandas etc, and pytorch, which we'll use to construct our deep learning models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "sns.set_style('white')\n",
    "plt.rcParams['figure.figsize'] = (20, 20)\n",
    "\n",
    "import os\n",
    "import io\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "from scipy.spatial.distance import cdist\n",
    "\n",
    "import torch\n",
    "from torch import nn, optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import models, transforms\n",
    "\n",
    "from tqdm._tqdm_notebook import tqdm_notebook as tqdm\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_path = '/mnt/efs/images/tiny-imagenet-200/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# wordvectors\n",
    "We're going to use the [fasttext](https://fasttext.cc/docs/en/english-vectors.html) word vectors trained on [common crawl](http://commoncrawl.org) as the target word vectors throughout this work. Let's load them into memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wv_path = '/mnt/efs/nlp/word_vectors/fasttext/crawl-300d-2M.vec'\n",
    "wv_file = io.open(wv_path, 'r', encoding='utf-8', newline='\\n', errors='ignore')\n",
    "\n",
    "fasttext = {line.split()[0]: np.array(line.split()[1:]).astype(np.float)\n",
    "            for line in tqdm(list(wv_file))}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocabulary = set(fasttext.keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# wordnet\n",
    "We're also going to need to load the wordnet classes and ids from tiny-imagenet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean = lambda x: x.lower().strip().replace(' ', '-').split(',-')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(base_path + 'wnids.txt') as f:\n",
    "    wnids = np.array([id.strip() for id in f.readlines()])\n",
    "\n",
    "wordnet = {}\n",
    "with open(base_path + 'words.txt') as f:\n",
    "    for line in f.readlines():\n",
    "        wnid, raw_words = line.split('\\t')\n",
    "        words = [word for word in clean(raw_words)\n",
    "                 if word in vocabulary]\n",
    "        \n",
    "        if wnid in wnids and len(words) > 0:\n",
    "            wordnet[wnid] = words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wnid_to_wordvector = {wnid: (np.array([fasttext[word] for word in words])\n",
    "                             .mean(axis=0))\n",
    "                      for wnid, words in wordnet.items()}\n",
    "\n",
    "wnids = list(wnid_to_wordvector.keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# example data\n",
    "here's an example of what we've got inside tiny-imagenet: one tiny image and its corresponding class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wnid = np.random.choice(wnids)\n",
    "image_path = base_path + 'train/' + wnid + '/images/' + wnid + '_{}.JPEG'\n",
    "print(' '.join(wordnet[wnid]))\n",
    "Image.open(image_path.format(np.random.choice(500)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# datasets and dataloaders\n",
    "Pytorch allows you to explicitly write out how batches of data are assembled and fed to a network. Especially when dealing with images, I've found it's best to use a pandas dataframe of simple paths and pointers as the base structure for assembling data. Instead of loading all of the images and corresponding word vectors into memory at once, we can just store the paths to the images with their wordnet ids. Using pandas also gives us the opportunity to do all sorts of work to the structure of the data without having to use much memory.  \n",
    "Here's how that dataframe is put together:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = {}\n",
    "\n",
    "for wnid in wnids:\n",
    "    wnid_path = base_path + 'train/' + wnid + '/images/'\n",
    "    image_paths = [wnid_path + file_name for file_name in os.listdir(wnid_path)]\n",
    "    for path in image_paths:\n",
    "        df[path] = wnid\n",
    "\n",
    "df = pd.Series(df).to_frame().reset_index()\n",
    "df.columns = ['path', 'wnid']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pandas is great for working with this kind of structured data - we can quickly shuffle the dataframe:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.sample(frac=1).reset_index(drop=True) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "and split it into 80:20 train:test portions. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "split_ratio = 0.8\n",
    "train_size = int(split_ratio * len(df))\n",
    "\n",
    "train_df = df.loc[:train_size]\n",
    "test_df  = df.loc[train_size:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "n.b. tiny-imagenet already has `train/`, `test/`, and `val/` directories set up which we could have used here instead. However, we're just illustrating the principle in this notebook so the data itself isn't important, and we'll use this kind of split later on when incorporating non-toy data.\n",
    "\n",
    "Now we can define how our `Dataset` object will transform the initial, simple data when it's called on to produce a batch. Images are generated by giving a path to `PIL`, and word vectors are looked up in our `wnid_to_wordvector` dictionary. Both objects are then transformed into pytorch tensors and handed over to the network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ImageDataset(Dataset):\n",
    "    def __init__(self, dataframe, wnid_to_wordvector,\n",
    "                 transform=transforms.ToTensor()):\n",
    "        self.image_paths = dataframe['path'].values\n",
    "        self.wnids = dataframe['wnid'].values\n",
    "        self.wnid_to_wordvector = wnid_to_wordvector\n",
    "        self.transform = transform\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        image = Image.open(self.image_paths[index]).convert('RGB')\n",
    "        if self.transform is not None:\n",
    "            image = self.transform(image)\n",
    "\n",
    "        target = torch.Tensor(wnid_to_wordvector[self.wnids[index]])\n",
    "        return image, target\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.wnids)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also apply transformations to the images as they move through the pipeline (see the `if` statement above in `__getitem__()`). The torchvision package provides lots of fast, intuitive utilities for this kind of thing which can be strung together as follows. Note that we're not applying any flips or grayscale to the test dataset - the test data should generally be left as raw as possible, with distortions applied at train time to increase the generality of the network's knowledge."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_transform = transforms.Compose([transforms.Resize(224),\n",
    "                                      transforms.RandomHorizontalFlip(),\n",
    "                                      transforms.RandomRotation(15),\n",
    "                                      transforms.RandomGrayscale(0.25),\n",
    "                                      transforms.ToTensor()])\n",
    "\n",
    "test_transform = transforms.Compose([transforms.Resize(224),\n",
    "                                     transforms.ToTensor()])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now all we need to do is pass our dataframe, dictionary of word vectors, and the desired image transforms to the `ImageDataset` object to define our data pipeline for training and testing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = ImageDataset(train_df, wnid_to_wordvector, train_transform)\n",
    "test_dataset = ImageDataset(test_df, wnid_to_wordvector, test_transform)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pytorch then requires that you pass the `Dataset` through a `DataLoader` to handle the batching etc. The `DataLoader` manages the pace and order of the work, while the `Dataset` does the work itself. The structure of these things is very predictable, and we don't have to write anything custom at this point."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 128\n",
    "\n",
    "train_loader = DataLoader(dataset=train_dataset,\n",
    "                          batch_size=batch_size,\n",
    "                          num_workers=5,\n",
    "                          shuffle=True)\n",
    "\n",
    "test_loader = DataLoader(dataset=test_dataset,\n",
    "                         batch_size=batch_size,\n",
    "                         num_workers=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# building the model\n",
    "Our model uses a pre-trained backbone to extract feature vectors from the images. This biases our network to perform well on imagenet-style images and worse on others, but hey, we're searching on imagenet in this example! Later on, when working in some less imagenet-y images, we'll make some attempts to compensate for the backbone's biases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "backbone = models.vgg16_bn(pretrained=True).features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We don't want this backbone to be trainable, so we switch off the gradients for its weight and bias tensors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for param in backbone.parameters():\n",
    "    param.requires_grad = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can put together the DeViSE network itself, which embeds image features into word vector space. The output of our backbone network is a $[512 \\times 7 \\times 7]$ tensor, which we then flatten into a 25088 dimensional vector. That vector is then fed through a few fully connected layers and ReLUs, while compressing the dimensionality down to our target size (300, to match the fasttext word vectors)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DeViSE(nn.Module):\n",
    "    def __init__(self, backbone, target_size=300):\n",
    "        super(DeViSE, self).__init__()\n",
    "        self.backbone = backbone\n",
    "        self.head = nn.Sequential(\n",
    "            nn.Linear(in_features=(25088), out_features=target_size*2),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Dropout(),\n",
    "            nn.Linear(in_features=target_size*2, out_features=target_size),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Dropout(),\n",
    "            nn.Linear(in_features=target_size, out_features=target_size),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.backbone(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.head(x)\n",
    "        x = x / x.max()\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "devise_model = DeViSE(backbone, target_size=300).to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# train loop\n",
    "Pytorch requires that we write our own training loops - this is rough skeleton structure that I've got used to. For each batch, the inputs and target tensors are first passed to the GPU. The inputs are then passed through the network to generate a set of predictions, which are compared to the target using some appropriate loss function. Those losses are used to inform the backpropagation of tweaks to the network's weights and biases, before repeating the whole process with a new batch. We also display the network's current loss through in the progress bar which tracks the speed and progress of the training. We can also specify the number of epochs in the parameters for the train function. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "losses = []\n",
    "flags = torch.ones(batch_size).cuda()\n",
    "\n",
    "def train(model, train_loader, loss_function, optimiser, n_epochs):\n",
    "    for epoch in range(n_epochs):\n",
    "        model.train()\n",
    "        loop = tqdm(train_loader)\n",
    "        for images, targets in loop:\n",
    "            images = images.cuda(non_blocking=True)\n",
    "            targets = targets.cuda(non_blocking=True)\n",
    "\n",
    "            optimiser.zero_grad()\n",
    "            predictions = model(images)\n",
    "\n",
    "            loss = loss_function(predictions, targets, flags)\n",
    "            loss.backward()\n",
    "            optimiser.step()\n",
    "\n",
    "            loop.set_description('Epoch {}/{}'.format(epoch + 1, n_epochs))\n",
    "            loop.set_postfix(loss=loss.item())\n",
    "            losses.append(loss.item())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we define the optimiser, loss function and learning rate which we'll use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainable_parameters = filter(lambda p: p.requires_grad, devise_model.parameters())\n",
    "\n",
    "loss_function = nn.CosineEmbeddingLoss()\n",
    "optimiser = optim.Adam(trainable_parameters, lr=0.001)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's do some training!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train(model=devise_model,\n",
    "      n_epochs=3,\n",
    "      train_loader=train_loader,\n",
    "      loss_function=loss_function,\n",
    "      optimiser=optimiser)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When that's done, we can take a look at how the losses are doing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_data = pd.Series(losses).rolling(window=15).mean()\n",
    "ax = loss_data.plot();\n",
    "\n",
    "ax.set_xlim(0,);\n",
    "ax.set_ylim(0, 1);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# evaluate on test set\n",
    "The loop below is very similar to the training one above, but evaluates the network's loss against the test set and stores the predictions. Obviously we're only going to loop over the dataset once here as we're not training anything. The network only has to see an image once to process it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = []\n",
    "test_loss = []\n",
    "flags = torch.ones(batch_size).cuda()\n",
    "\n",
    "devise_model.eval()\n",
    "with torch.no_grad():\n",
    "    test_loop = tqdm(test_loader)\n",
    "    for images, targets in test_loop:\n",
    "        images = images.cuda(non_blocking=True)\n",
    "        targets = targets.cuda(non_blocking=True)\n",
    "\n",
    "        predictions = devise_model(images)\n",
    "        loss = loss_function(predictions, targets, flags)\n",
    "\n",
    "        preds.append(predictions.cpu().data.numpy())\n",
    "        test_loss.append(loss.item())\n",
    "\n",
    "        test_loop.set_description('Test set')\n",
    "        test_loop.set_postfix(loss=np.mean(test_loss[-5:]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = np.concatenate(preds).reshape(-1, 300)\n",
    "np.mean(test_loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# run a search on the predictions\n",
    "Now we're ready to use our network to perform image searches! Each of the test set's images has been assigned a position in word vector space which the network believes is a reasonable numeric description of its features. We can use the complete fasttext dictionary to find the position of new, unseen words, and then return the nearest images to our query."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def search(query, n=5):\n",
    "    image_paths = test_df['path'].values\n",
    "    distances = cdist(fasttext[query].reshape(1, -1), preds)\n",
    "    closest_n_paths = image_paths[np.argsort(distances)].squeeze()[:n]\n",
    "    close_images = [np.array(Image.open(image_path).convert('RGB')) \n",
    "                    for image_path in closest_n_paths]\n",
    "    return Image.fromarray(np.concatenate(close_images, axis=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "search('bridge')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It works! The network has never seen the word 'bridge', has never been told what a bridge might look like, and has never seen any of the test set's images, but thanks to the combined subtlety of the word vector space which we're embedding our images in and the dexterity with which a neural network can manipulate manifolds like these, the machine has enough knowledge to make a very good guess at what a bridge might be. This has been trained on a tiny, terribly grainy set of data but it's enough to get startlingly good results."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
