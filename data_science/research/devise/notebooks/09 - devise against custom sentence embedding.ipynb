{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# devise against custom sentence embedding\n",
    "Now that we have a method of creating reasonably good sentence embeddings, we need to learn the mapping from sentence embedding space to image feature vector space. This process is essentially the same as what we've demonstrated before."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "sns.set_style('whitegrid')\n",
    "plt.rcParams['figure.figsize'] = (20, 14)\n",
    "\n",
    "import os\n",
    "import json\n",
    "import nltk\n",
    "import pickle\n",
    "import itertools\n",
    "import numpy as np \n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "from scipy.spatial.distance import cdist\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "from tqdm import tqdm as tqdm_\n",
    "tqdm_.pandas()\n",
    "\n",
    "import io\n",
    "from nltk.tokenize import word_tokenize\n",
    "from sklearn.preprocessing import LabelEncoder, OneHotEncoder\n",
    "\n",
    "import torch\n",
    "from torch import nn, optim\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "from torchvision import models, transforms\n",
    "\n",
    "import spacy\n",
    "nlp = spacy.load('en')\n",
    "\n",
    "nltk.download('punkt')\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# load data\n",
    "We'll load in some of the bits of data we saved at the end of the last notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "index_to_wordvec = np.load('/mnt/efs/models/index_to_wordvec.npy')\n",
    "word_to_index = pickle.load(open('/mnt/efs/models/word_to_index.pkl', 'rb'))\n",
    "index_to_word = pickle.load(open('/mnt/efs/models/index_to_word.pkl', 'rb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# utils\n",
    "and define a few utility functions which will come in handy later on. The all contribute to being able to embed a query sentence with a single function call."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sentence_to_indexes(sentence):\n",
    "    sentence = sentence.lower()\n",
    "    tokenised = word_tokenize(sentence)\n",
    "    indexes = [word_to_index[word] \n",
    "               for word in tokenised \n",
    "               if word in word_to_index]\n",
    "    return indexes\n",
    "\n",
    "\n",
    "def embed(sentence):\n",
    "    indexes = ([word_to_index['<s>']] + \n",
    "               sentence_to_indexes(sentence) +\n",
    "               [word_to_index['</s>']])\n",
    "    wvs = np.stack([index_to_wordvec[i] for i in indexes])\n",
    "    embedding = model(torch.Tensor([wvs]).cuda()).cpu().data.numpy()\n",
    "    return embedding.squeeze()\n",
    "\n",
    "def embed_paragraph(paragraph):\n",
    "    sentences = nltk.sent_tokenize(paragraph)\n",
    "    if len(sentences) == 0:\n",
    "        embeddings = embed('.')\n",
    "    else:\n",
    "        embeddings = [embed(sentence) for sentence in sentences]\n",
    "    return np.array(embeddings).max(axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# sentence embedding models\n",
    "We define the sentence embedding model in exactly the same way as we did before so that its learned weights from the last notebook can be overlaid without any issues."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hidden_size = 2048\n",
    "\n",
    "class SentenceEncoder(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SentenceEncoder, self).__init__()\n",
    "        self.enc_lstm = nn.LSTM(input_size=300, \n",
    "                                hidden_size=hidden_size, \n",
    "                                num_layers=1,\n",
    "                                bidirectional=True)\n",
    "        \n",
    "    def forward(self, wv_batch):\n",
    "        embedded, _ = self.enc_lstm(wv_batch)\n",
    "        max_pooled = torch.max(embedded, 1)[0] \n",
    "        return max_pooled\n",
    "\n",
    "\n",
    "class NLINet(nn.Module):\n",
    "    def __init__(self, index_to_wordvec):\n",
    "        super(NLINet, self).__init__()\n",
    "        self.index_to_wordvec = index_to_wordvec\n",
    "        self.encoder = SentenceEncoder()\n",
    "        self.classifier = nn.Sequential(nn.Dropout(0.2),\n",
    "                                        nn.Linear(hidden_size*8, 128),\n",
    "                                        nn.ReLU(),\n",
    "                                        nn.Dropout(0.2),\n",
    "                                        nn.Linear(128, 3),\n",
    "                                       )\n",
    "\n",
    "    def forward(self, s1, s2):\n",
    "        u, v = self.encoder(s1), self.encoder(s2)\n",
    "        features = torch.cat((u, v, torch.abs(u - v), u * v), 1)\n",
    "        return self.classifier(features)\n",
    "\n",
    "    def encode(self, sentences):\n",
    "        indexes = sentence_to_indexes(sentences)\n",
    "        wvs = torch.Tensor(np.stack([self.index_to_wordvec[i] for i in indexes]))\n",
    "        return self.encoder([wvs])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = SentenceEncoder().to(device)\n",
    "\n",
    "model_path = '/mnt/efs/models/sentence-encoder-2018-10-08.pt'\n",
    "model.load_state_dict(torch.load(model_path))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# get image data\n",
    "We can now start loading in the image data we'll use to learn mappings from image to sentence space. This follows the same pattern as before - building a pandas dataframe of paths and captions, whose embeddings and images can be calculated or looked up at train time for each batch, as needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wellcome_image_path = '/mnt/efs/images/wellcome_images/'\n",
    "\n",
    "wellcome_image_paths = [wellcome_image_path + subdir + '/' + wellcome_image_id\n",
    "                        for subdir in os.listdir(wellcome_image_path)\n",
    "                        for wellcome_image_id in os.listdir(wellcome_image_path+subdir)]\n",
    "\n",
    "wellcome_image_ids = [path.split('/')[-1].split('.')[0] for path in wellcome_image_paths]\n",
    "\n",
    "wellcome_path_series = pd.Series(dict(zip(wellcome_image_ids, wellcome_image_paths)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "meta = pd.read_json('/mnt/efs/other/works.json', lines=True)\n",
    "meta.index = meta['identifiers'].apply(lambda x: x[0]['value']).rename()\n",
    "wellcome_title_series = meta['title'].fillna('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wellcome_df = pd.concat([wellcome_path_series, wellcome_title_series], axis=1)\n",
    "wellcome_df.columns = ['file_name', 'caption']\n",
    "\n",
    "wellcome_df = wellcome_df.dropna()\n",
    "wellcome_df['caption'] = wellcome_df['caption']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('/mnt/efs/images/coco/annotations/captions_val2014.json') as f:\n",
    "    meta = json.load(f)\n",
    "\n",
    "coco_df = (pd.merge(pd.DataFrame(meta['images']).set_index('id'),\n",
    "                    pd.DataFrame(meta['annotations']).set_index('image_id'), \n",
    "                    left_index=True, right_index=True)\n",
    "           .reset_index()\n",
    "           [['caption', 'file_name']]\n",
    "          )\n",
    "\n",
    "coco_df['file_name'] = '/mnt/efs/images/coco/val2014/' + coco_df['file_name']\n",
    "\n",
    "coco_df['caption'] = (coco_df['caption']\n",
    "                      .apply(lambda x: ''.join([c for c in x if c.isalpha() or c.isspace()]))\n",
    "                      .apply(str.lower)\n",
    "                      .apply(lambda x: ' '.join(x.split()))\n",
    "                     )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del wellcome_image_paths\n",
    "del wellcome_path_series\n",
    "del wellcome_title_series\n",
    "del meta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# find nouns and adjective-noun pairs in sentences\n",
    "We want to double down on the inclusion and good interpretation of short sequences, so we'll preprocess this core dataframe to find nouns and adjective-noun pairs too, matching them with the same image paths as their source sequences. This new short-sequence dataframe will then be appended to the original."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "source_data = (pd.concat([wellcome_df, coco_df])\n",
    "               .fillna('')\n",
    "               .sample(50000)\n",
    "               .values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We extract the nouns from the sequence (using spacy's POS tagger) and add them to a dictionary, paired with their original image path. We'll also grab any adjective-noun pairs while we're there."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subjects, i = {}, 0\n",
    "\n",
    "for caption, path in tqdm(source_data):\n",
    "    words = nlp(caption)\n",
    "    \n",
    "    for word in words:\n",
    "        if word.pos_ == 'NOUN':\n",
    "            subjects[i] = {'caption': word.text,\n",
    "                           'file_name': path}\n",
    "            i += 1\n",
    "    \n",
    "    for i in range(len(words) - 1):\n",
    "        word_1, word_2 = words[i:i+2]\n",
    "        if ((word_1.pos_ == 'ADJ') & (word_2.pos_ == 'NOUN')):\n",
    "            subjects[i] = {'caption': ' '.join([word_1.text, word_2.text]),\n",
    "                           'file_name': path}\n",
    "            i += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll now transform that dictionary into a dataframe so that it can be combined with the ones we loaded in before."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subjects = pd.DataFrame(subjects).T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# train test splits\n",
    "We'll train on the coco and wellcome data combined (with their short sequence counterparts), and test against just the original wellcome captions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask = np.random.rand(len(wellcome_df)) < 0.8\n",
    "train_df = pd.concat([wellcome_df[mask], coco_df, subjects], axis=0)\n",
    "test_df = wellcome_df[~mask]\n",
    "\n",
    "len(train_df), len(test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del coco_df\n",
    "del source_data\n",
    "del subjects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_embeddings = np.array([embed(caption) for caption in tqdm(train_df['caption'])])\n",
    "test_embeddings = np.array([embed(caption) for caption in tqdm(test_df['caption'])])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# datasets and dataloaders\n",
    "### dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CaptionsDataset(Dataset):\n",
    "    def __init__(self, path_df, caption_embeddings, \n",
    "                 transform=transforms.ToTensor()):\n",
    "        self.ids = path_df.index.values\n",
    "        self.image_paths = path_df['file_name'].values\n",
    "        self.caption_embeddings = caption_embeddings\n",
    "        self.transform = transform\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        image = Image.open(self.image_paths[index]).convert('RGB')\n",
    "        if self.transform is not None:\n",
    "            image = self.transform(image)\n",
    "\n",
    "        target = self.caption_embeddings[index]\n",
    "        return image, target\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_transform = transforms.Compose([transforms.RandomResizedCrop(224, scale=[0.65, 0.9]),\n",
    "                                      transforms.RandomHorizontalFlip(),\n",
    "                                      transforms.RandomGrayscale(0.35),\n",
    "                                      transforms.ToTensor()])\n",
    "\n",
    "test_transform = transforms.Compose([transforms.RandomResizedCrop(224, scale=[0.65, 0.9]),\n",
    "                                     transforms.ToTensor()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = CaptionsDataset(train_df, train_embeddings, transform=train_transform)\n",
    "test_dataset = CaptionsDataset(test_df, test_embeddings, transform=test_transform)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 64\n",
    "\n",
    "train_loader = DataLoader(dataset=train_dataset, \n",
    "                          batch_size=batch_size,\n",
    "                          shuffle=True,\n",
    "                          num_workers=5)\n",
    "\n",
    "test_loader = DataLoader(dataset=test_dataset, \n",
    "                         batch_size=batch_size,\n",
    "                         num_workers=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# create DeViSE model\n",
    "This is the same devise model as we've seen before, with the only difference being the target size. Our sentence space is now 4096d, rather than 300d, and our model adapts accordingly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "backbone = models.vgg16_bn(pretrained=True).features\n",
    "\n",
    "for param in backbone[:37].parameters():\n",
    "    param.requires_grad = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DeViSE(nn.Module):\n",
    "    '''\n",
    "    learn to map images into sentence space\n",
    "    '''\n",
    "    def __init__(self, backbone, target_size):\n",
    "        super(DeViSE, self).__init__()\n",
    "        self.backbone = backbone\n",
    "        self.head = nn.Sequential(\n",
    "            nn.Linear(in_features=512*7*7, out_features=target_size),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Dropout(),\n",
    "            nn.Linear(in_features=target_size, out_features=target_size),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.backbone(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.head(x)\n",
    "        x = x / x.max()  # normalise the output to keep mse sane\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "devise_model = DeViSE(backbone, target_size=4096).to(device)\n",
    "devise_model_path = '/mnt/efs/models/devise-2018-10-09.pt'\n",
    "devise_model.load_state_dict(torch.load(devise_model_path))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# train\n",
    "Let's do some training!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "losses = []\n",
    "\n",
    "def train(model, train_loader, n_epochs, loss_function, \n",
    "          additional_metric, optimiser, device=device):\n",
    "    for epoch in range(n_epochs):\n",
    "        model.train()\n",
    "        loop = tqdm(train_loader)\n",
    "        for data, target in loop:\n",
    "            data, target, flags = (data.cuda(non_blocking=True), \n",
    "                                   target.cuda(non_blocking=True), \n",
    "                                   torch.ones(len(target)).cuda(non_blocking=True))\n",
    "\n",
    "            optimiser.zero_grad()\n",
    "            prediction = model(data)\n",
    "\n",
    "            loss = loss_function(prediction, target, flags)\n",
    "            mean_sq_error = additional_metric(prediction, target)\n",
    "            losses.append([loss.item(), mean_sq_error.item()])\n",
    "\n",
    "            loss.backward()\n",
    "            optimiser.step()\n",
    "\n",
    "            loop.set_description('Epoch {}/{}'.format(epoch + 1, n_epochs))\n",
    "            loop.set_postfix(loss=loss.item(), mse=mean_sq_error.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.backends.cudnn.benchmark = True\n",
    "loss_function, mse = nn.CosineEmbeddingLoss(), nn.MSELoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainable_parameters = filter(lambda p: p.requires_grad, devise_model.parameters())\n",
    "optimiser = optim.Adam(trainable_parameters, lr=0.0001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train(model=devise_model,\n",
    "      train_loader=train_loader,\n",
    "      loss_function=loss_function,\n",
    "      additional_metric=mse, \n",
    "      optimiser=optimiser,\n",
    "      n_epochs=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_data = pd.DataFrame(losses).rolling(window=15).mean()\n",
    "loss_data.columns = ['cosine loss', 'mse']\n",
    "ax = loss_data.plot(subplots=True);\n",
    "\n",
    "ax[0].set_xlim(0,);\n",
    "ax[0].set_ylim(0, 0.6);\n",
    "ax[1].set_ylim(0,);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# predict\n",
    "and make some predictions on the test set, checking our loss metric along the way."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = []\n",
    "test_loss = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    test_loop = tqdm(test_loader)\n",
    "    for data, target in test_loop:\n",
    "        data, target, flags = (data.cuda(),\n",
    "                               target.cuda(),\n",
    "                               torch.ones(len(target)).cuda())\n",
    "\n",
    "        prediction = devise_model.eval()(data)\n",
    "        loss = loss_function(prediction, target, flags)\n",
    "\n",
    "        preds.append(prediction.cpu().data.numpy())\n",
    "        test_loss.append(loss.item())\n",
    "\n",
    "        test_loop.set_description('Test set')\n",
    "        test_loop.set_postfix(loss=loss.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = np.concatenate(preds).reshape(-1, 4096)\n",
    "np.mean(test_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings = preds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# search\n",
    "We can now run proper searches against our wellcome data! We'll brute force the search here, but in the real world we'll precompute a search index using `nmslib`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def search(query):\n",
    "    query_embedding = embed(query).reshape(-1, 4096)\n",
    "\n",
    "    distances = cdist(query_embedding, embeddings, 'cosine').squeeze()\n",
    "    nearby_image_paths = test_df['file_name'].values[np.argsort(distances)][:20]\n",
    "    nearby_images = [np.array((Image.open(path)\n",
    "                               .convert('RGB')\n",
    "                               .resize((224, 224), Image.BILINEAR)))\n",
    "                     for path in nearby_image_paths]\n",
    "\n",
    "    return Image.fromarray(np.concatenate([np.concatenate(nearby_images[:5], axis=1),\n",
    "                                           np.concatenate(nearby_images[5:10], axis=1),\n",
    "                                           np.concatenate(nearby_images[10:15], axis=1),\n",
    "                                           np.concatenate(nearby_images[15:20], axis=1)],\n",
    "                                          axis=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "search('simulations of protein structure')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "search('text written in hindi')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "search('photograph of stone pillars in a church')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "search('portrait of a man')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "search('portrait of a woman')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "search('mri scan of a brain')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "search('pretty drawings of plants and flowers')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "search('really horrible , disgusting drawings of burns and skin diseases')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "search('surgical instruments')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "search('astronomical charts of the moons')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "search('a cat preparing for surgery')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "search('dancing skeletons')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "search('giraffe')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "search('a man dancing')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "search('a collection of blood cells')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "search('a waterfall')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "search('anatomical details of the tendons in hands and fingers')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "search('buddhist man sitting with folded legs')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "search('AIDS posters')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "search('fractured bone')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(devise_model.state_dict(), '/mnt/efs/models/devise-2018-10-09.pt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# save devise'd embeddings\n",
    "We want to use these embeddings in our demo app, so we'll save them here so that they can be moved over to the app's data directory by hand."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_caption_embeddings = np.array([embed(caption) for caption in tqdm(wellcome_df['caption'].values)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_dataset = CaptionsDataset(wellcome_df, all_caption_embeddings, transform=test_transform)\n",
    "full_loader = DataLoader(dataset=full_dataset, \n",
    "                         batch_size=batch_size,\n",
    "                         num_workers=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    loop = tqdm(full_loader)\n",
    "    for data, target in loop:\n",
    "        data, target, flags = (data.cuda(),\n",
    "                               target.cuda(),\n",
    "                               torch.ones(len(target)).cuda())\n",
    "\n",
    "        embedding = devise_model.eval()(data)\n",
    "        embeddings.append(embedding.cpu().data.numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings = np.concatenate(embeddings).reshape(-1, 1024)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_to_id = lambda x: x.split('/')[-1].split('.')[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save('/mnt/efs/models/embeddings.npy', embeddings)\n",
    "np.save('/mnt/efs/models/image_ids.npy', wellcome_df['file_name'].apply(path_to_id).values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
